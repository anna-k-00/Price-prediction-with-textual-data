{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP9y2-osPyWI",
        "outputId": "3b73b22a-8bfa-4049-958d-6b818d2c2391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Price-prediction-with-textual-data'...\n",
            "remote: Enumerating objects: 308, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (190/190), done.\u001b[K\n",
            "remote: Total 308 (delta 114), reused 6 (delta 6), pack-reused 111 (from 1)\u001b[K\n",
            "Receiving objects: 100% (308/308), 1.80 MiB | 25.64 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade transformers==4.49.0\n",
        "# !pip install optuna==2.10.0\n",
        "# !pip install numpy==1.26.4 gensim==4.3.2\n",
        "# !pip install scipy==1.12.0\n",
        "# !pip install --upgrade pandas==2.2.2\n",
        "# !pip install h3\n",
        "# !pip install mlflow\n",
        "# !pip install 'protobuf<4'\n",
        "# !pip install selenium\n",
        "# !pip install natasha\n",
        "# !pip install pymystem3\n",
        "# !pip install symspellpy\n",
        "!rm -rf /content/Price-prediction-with-textual-data\n",
        "!git clone https://github.com/anna-k-00/Price-prediction-with-textual-data.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 1: Проверка и настройка окружения\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# Шаг 2: Клонирование/обновление репозитория\n",
        "repo_url = 'https://github.com/anna-k-00/Price-prediction-with-textual-data.git'\n",
        "repo_dir = 'Price-prediction-with-textual-data'\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    !cd {repo_dir} && git pull\n",
        "\n",
        "# Шаг 3: Добавляем все нужные пути в sys.path\n",
        "paths_to_add = [\n",
        "    f'/content/{repo_dir}',                     # Для файлов в корне (parser_avito.py)\n",
        "    f'/content/{repo_dir}/main_methods',        # Основные модули\n",
        "    f'/content/{repo_dir}/embeddings_generation', # Генерация эмбеддингов\n",
        "    f'/content/{repo_dir}/preprocessors'        # Препроцессоры\n",
        "]\n",
        "\n",
        "for path in paths_to_add:\n",
        "    if os.path.exists(path) and path not in sys.path:\n",
        "        sys.path.insert(0, path)\n",
        "        print(f'Добавлен путь: {path}')\n",
        "\n",
        "# Шаг 4: Собираем список всех модулей для импорта\n",
        "all_modules = [\n",
        "    # Основные модули\n",
        "    'resource_monitor', 'ANN', 'predict', 'test_pipeline',\n",
        "\n",
        "    # Модули из embeddings_generation\n",
        "    'embeddings_generation.rubert_fine_tuning',\n",
        "    'embeddings_generation.tfidf_generator',\n",
        "    'embeddings_generation.w2v_generator',\n",
        "    'embeddings_generation.gate',\n",
        "\n",
        "    # Модули из preprocessors\n",
        "    'preprocessors.preprocessor_params_hex',\n",
        "    'preprocessors.preprocessor_text',\n",
        "\n",
        "    # Отдельные файлы в корне\n",
        "    'parser_avito'\n",
        "]\n",
        "\n",
        "# Шаг 5: Импортируем все модули\n",
        "imported_modules = {}\n",
        "failed_modules = {}\n",
        "\n",
        "for module_name in all_modules:\n",
        "    try:\n",
        "        module = importlib.import_module(module_name)\n",
        "        imported_modules[module_name] = module\n",
        "        print(f'✅ {module_name} успешно импортирован')\n",
        "    except Exception as e:\n",
        "        failed_modules[module_name] = str(e)\n",
        "        print(f'❌ Ошибка импорта {module_name}: {str(e)[:200]}')  # Обрезаем длинные сообщения"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaKe5niAZQuS",
        "outputId": "ff10a9fd-5b79-46a2-967e-766d4024aff1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "Добавлен путь: /content/Price-prediction-with-textual-data\n",
            "Добавлен путь: /content/Price-prediction-with-textual-data/main_methods\n",
            "Добавлен путь: /content/Price-prediction-with-textual-data/embeddings_generation\n",
            "Добавлен путь: /content/Price-prediction-with-textual-data/preprocessors\n",
            "✅ resource_monitor успешно импортирован\n",
            "✅ ANN успешно импортирован\n",
            "✅ predict успешно импортирован\n",
            "✅ test_pipeline успешно импортирован\n",
            "✅ embeddings_generation.rubert_fine_tuning успешно импортирован\n",
            "✅ embeddings_generation.tfidf_generator успешно импортирован\n",
            "✅ embeddings_generation.w2v_generator успешно импортирован\n",
            "✅ embeddings_generation.gate успешно импортирован\n",
            "✅ preprocessors.preprocessor_params_hex успешно импортирован\n",
            "✅ preprocessors.preprocessor_text успешно импортирован\n",
            "✅ parser_avito успешно импортирован\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msafR8KFe4qO",
        "outputId": "1f04ac10-2a7f-4105-9ed8-246a921ce482"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from parser_avito import AvitoParser\n",
        "import pandas as pd\n",
        "\n",
        "# it's important thatt from google colaboratory parser will not work so it is better to run locally\n",
        "# process = AvitoParser(\n",
        "#                       target_types = ['house'], # if empty parses all categories\n",
        "#                       target_highways = ['Фряновское шоссе'], # if empty parses all highways\n",
        "#                        path_links='raw_links_house.txt',\n",
        "#                       df_path='data_new_house.csv',\n",
        "#                       drop_prev_files=False,\n",
        "#                       parse_new_links=True\n",
        "#                       )\n",
        "# process.initializer()\n",
        "\n",
        "# df = pd.read_csv('data_new_house.csv')"
      ],
      "metadata": {
        "id": "_VTiN1erciSc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/thesis/data_new_house.csv')"
      ],
      "metadata": {
        "id": "B-4mqqeleoCh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #preprocess text\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# # Для русского языка дополнительно:\n",
        "# try:\n",
        "#     nltk.data.find('tokenizers/punkt/russian.pickle')\n",
        "# except LookupError:\n",
        "#     nltk.download('punkt_tab')  # Специфичные данные для русского языка\n",
        "\n",
        "# # !wget http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n",
        "# # !tar -xvzf mystem-3.1-linux-64bit.tar.gz\n",
        "# # !chmod +x mystem\n",
        "# # !mv mystem /usr/local/bin/"
      ],
      "metadata": {
        "id": "o_zbItzsfaO7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessor_text import TextPreprocessor\n",
        "\n",
        "process = TextPreprocessor(df,\n",
        "                  text_columns = ['description'],\n",
        "                  fix_spelling = True,\n",
        "                 lemmatize_text = True,\n",
        "                 remove_stopwords = True,\n",
        "                 remove_punctuation = True,\n",
        "                             )\n",
        "df_text_prep = process.process_dataframe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KScr8ywgteI",
        "outputId": "e412437f-5a9c-4766-98b6-c6acda74f263"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing description: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is how final df is gathered\n",
        "# description_raw left raw for rubert\n",
        "\n",
        "df = df.rename(columns = {'description':'description_raw'})\n",
        "df['description'] = df_text_prep['description']\n"
      ],
      "metadata": {
        "id": "HYemM0OmgJWq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(r'/content/drive/MyDrive/thesis/support/sample_100.csv')"
      ],
      "metadata": {
        "id": "V96VV_ZuiNhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
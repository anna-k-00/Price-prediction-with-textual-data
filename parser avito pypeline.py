{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42aa7bfb-25b1-4f5e-9ca2-020084b15d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm  \n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import concurrent.futures\n",
    "import random \n",
    "\n",
    "# Define property types and highways\n",
    "property_types = {'dom': 'ASgBAgICAkSUA9AQ2AjOWQ',\n",
    "                  'dacha': 'ASgBAgICAkSUA9AQ2AjQWQ',\n",
    "                  'kottedzh': 'ASgBAgICAkSUA9AQ2AjKWQ'}\n",
    "\n",
    "highways = {\n",
    "    1: 'Алтуфьевское шоссе',\n",
    "    2: 'Боровское шоссе',\n",
    "    3: 'Быковское шоссе',\n",
    "    4: 'Варшавское шоссе',\n",
    "    5: 'Волоколамское шоссе',\n",
    "    6: 'Горьковское шоссе',\n",
    "    7: 'Дмитровское шоссе',\n",
    "    8: 'Егорьевское шоссе',\n",
    "    9: 'Ильинское шоссе',\n",
    "    10: 'Калужское шоссе',\n",
    "    11: 'Каширское шоссе',\n",
    "    12: 'Киевское шоссе',\n",
    "    13: 'Куркинское шоссе',\n",
    "    14: 'Ленинградское шоссе',\n",
    "    15: 'Минское шоссе',\n",
    "    16: 'Можайское шоссе',\n",
    "    17: 'Новокаширское шоссе',\n",
    "    18: 'Новорижское шоссе',\n",
    "    19: 'Новорязанское шоссе',\n",
    "    20: 'Новосходненское шоссе',\n",
    "    21: 'Носовихинское шоссе',\n",
    "    22: 'Осташковское шоссе',\n",
    "    23: 'Пятницкое шоссе',\n",
    "    24: 'Рогачёвское шоссе',\n",
    "    25: 'Рублёво-Успенское шоссе',\n",
    "    26: 'Рублёвское шоссе',\n",
    "    27: 'Рязанское шоссе',\n",
    "    28: 'Симферопольское шоссе',\n",
    "    29: 'Сколковское шоссе',\n",
    "    30: 'Фряновское шоссе',\n",
    "    31: 'Щёлковское шоссе',\n",
    "    32: 'Ярославское шоссе',\n",
    "}\n",
    "\n",
    "class AvitoParser:\n",
    "    def __init__(self, \n",
    "                 target_types=list(property_types.keys()), \n",
    "                 target_highways=list(highways.values()), \n",
    "                 path_links='raw_links_house.txt',\n",
    "                 df_path='data_new_house.csv',\n",
    "                 drop_prev_files=False,\n",
    "                 parse_new_links=True):\n",
    "        \"\"\"\n",
    "        Initialize the AvitoParser class with the given parameters.\n",
    "\n",
    "        :param target_types: List of property types to parse.\n",
    "        :param target_highways: List of highways to parse.\n",
    "        :param path_links: Path to the file where raw links will be stored.\n",
    "        :param df_path: Path to the CSV file where parsed data will be stored.\n",
    "        :param drop_prev_files: Whether to drop previous files.\n",
    "        :param parse_new_links: Whether to parse new links.\n",
    "        \"\"\"\n",
    "        self.target_types = target_types\n",
    "        self.target_highways = target_highways\n",
    "        self.path_links = path_links\n",
    "        self.df_path = df_path\n",
    "        self.drop_prev_files = drop_prev_files\n",
    "        self.parse_new_links = parse_new_links\n",
    "\n",
    "        self.property_types = {'dom': 'ASgBAgICAkSUA9AQ2AjOWQ',\n",
    "                  'dacha': 'ASgBAgICAkSUA9AQ2AjQWQ',\n",
    "                  'kottedzh': 'ASgBAgICAkSUA9AQ2AjKWQ'}\n",
    "\n",
    "        self.highways = {\n",
    "            1: 'Алтуфьевское шоссе',\n",
    "            2: 'Боровское шоссе',\n",
    "            3: 'Быковское шоссе',\n",
    "            4: 'Варшавское шоссе',\n",
    "            5: 'Волоколамское шоссе',\n",
    "            6: 'Горьковское шоссе',\n",
    "            7: 'Дмитровское шоссе',\n",
    "            8: 'Егорьевское шоссе',\n",
    "            9: 'Ильинское шоссе',\n",
    "            10: 'Калужское шоссе',\n",
    "            11: 'Каширское шоссе',\n",
    "            12: 'Киевское шоссе',\n",
    "            13: 'Куркинское шоссе',\n",
    "            14: 'Ленинградское шоссе',\n",
    "            15: 'Минское шоссе',\n",
    "            16: 'Можайское шоссе',\n",
    "            17: 'Новокаширское шоссе',\n",
    "            18: 'Новорижское шоссе',\n",
    "            19: 'Новорязанское шоссе',\n",
    "            20: 'Новосходненское шоссе',\n",
    "            21: 'Носовихинское шоссе',\n",
    "            22: 'Осташковское шоссе',\n",
    "            23: 'Пятницкое шоссе',\n",
    "            24: 'Рогачёвское шоссе',\n",
    "            25: 'Рублёво-Успенское шоссе',\n",
    "            26: 'Рублёвское шоссе',\n",
    "            27: 'Рязанское шоссе',\n",
    "            28: 'Симферопольское шоссе',\n",
    "            29: 'Сколковское шоссе',\n",
    "            30: 'Фряновское шоссе',\n",
    "            31: 'Щёлковское шоссе',\n",
    "            32: 'Ярославское шоссе',\n",
    "        }\n",
    "\n",
    "\n",
    "    def initializer(self, target_types=None, target_highways=None, \n",
    "                    path_links=None, df_path=None, \n",
    "                    drop_prev_files=None, parse_new_links=None):\n",
    "        \"\"\"\n",
    "        Initialize the parsing process with optional parameters.\n",
    "\n",
    "        :param target_types: List of property types to parse.\n",
    "        :param target_highways: List of highways to parse.\n",
    "        :param path_links: Path to the file where raw links will be stored.\n",
    "        :param df_path: Path to the CSV file where parsed data will be stored.\n",
    "        :param drop_prev_files: Whether to drop previous files.\n",
    "        :param parse_new_links: Whether to parse new links.\n",
    "        \"\"\"\n",
    "        # Update class attributes if new values are provided\n",
    "        if target_types is not None:\n",
    "            self.target_types = target_types\n",
    "        if target_highways is not None:\n",
    "            self.target_highways = target_highways\n",
    "        if path_links is not None:\n",
    "            self.path_links = path_links\n",
    "        if df_path is not None:\n",
    "            self.df_path = df_path\n",
    "        if drop_prev_files is not None:\n",
    "            self.drop_prev_files = drop_prev_files\n",
    "        if parse_new_links is not None:\n",
    "            self.parse_new_links = parse_new_links  # Ensure this is updated\n",
    "\n",
    "        # Debugging: Print the updated attributes\n",
    "        print(f\"Updated attributes: target_types={self.target_types}, parse_new_links={self.parse_new_links}\")\n",
    "\n",
    "        # Generate tuples of property types and highways\n",
    "        tuples = []\n",
    "        hw_numbers = [k for k, v in highways.items() if v in self.target_highways]\n",
    "        for type in self.target_types:\n",
    "            for highway in hw_numbers:\n",
    "                tuples.append((type, highway))\n",
    "\n",
    "        # Start the parsing process\n",
    "        self.get_dist_ads_urls(tuples)\n",
    "\n",
    "    def get_dist_ads_urls(self, tuples):\n",
    "        \"\"\"\n",
    "        Get the URLs of the ads based on the tuples of property types and highways.\n",
    "\n",
    "        :param tuples: List of tuples containing property types and highways.\n",
    "        \"\"\"\n",
    "        counter_ads = 0\n",
    "        chrome_options = Options()\n",
    "        prefs = {\n",
    "            \"profile.managed_default_content_settings.images\": 2,  # 2 means block images\n",
    "            \"profile.default_content_setting_values.images\": 2,    # 2 means block images\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "        if self.drop_prev_files:\n",
    "            if os.path.exists(self.path_links):\n",
    "                os.remove(self.path_links)\n",
    "            if os.path.exists(self.df_path):\n",
    "                os.remove(self.df_path)\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(), options=chrome_options)\n",
    "        print(self.parse_new_links)\n",
    "        if self.parse_new_links:\n",
    "            print('getting urls')\n",
    "            for type, highway in tqdm(tuples, desc=\"Processing highways\", unit=\"highway\"):\n",
    "                try:\n",
    "                    url = f'https://www.avito.ru/moskovskaya_oblast/doma_dachi_kottedzhi/prodam/{type}-{property_types[type]}?context=H4sIAAAAAAAA_wEjANz_YToxOntzOjg6ImZyb21QYWdlIjtzOjc6ImNhdGFsb2ciO312FITcIwAAAA&road={highway}'\n",
    "                    driver.get(url)\n",
    "                    ads_count = int(driver.find_element(by=By.XPATH, value=\"//span[@data-marker='page-title/count']\").text.replace(' ', ''))\n",
    "                    counter_ads += ads_count\n",
    "                    if ads_count % 50 > 0:\n",
    "                        page_count = (ads_count // 50) + 1\n",
    "                    else:\n",
    "                        page_count = ads_count // 50\n",
    "\n",
    "                    for page in range(1, page_count + 1):\n",
    "                        if page == 1:\n",
    "                            ads_elements = driver.find_elements(by=By.XPATH, value='//a[@data-marker=\"item-title\"]')\n",
    "                        else:\n",
    "                            driver.get(f\"https://www.avito.ru/moskovskaya_oblast/doma_dachi_kottedzhi/prodam/{type}-{property_types[type]}?context=H4sIAAAAAAAA_wEjANz_YToxOntzOjg6ImZyb21QYWdlIjtzOjc6ImNhdGFsb2ciO312FITcIwAAAA&p={page}&road={highway}\")\n",
    "                            ads_elements = driver.find_elements(by=By.XPATH, value='//a[@data-marker=\"item-title\"]')\n",
    "\n",
    "                        with open(self.path_links, mode='a') as f:\n",
    "                            for ad in ads_elements:\n",
    "                                link = ad.get_attribute(\"href\")\n",
    "                                if link:\n",
    "                                    f.write(link + '\\n')\n",
    "                                print(link)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            driver.quit()\n",
    "            # print('Total number of ads to parse: ', counter_ads)\n",
    "\n",
    "        # Use ThreadPoolExecutor to parallelize the parsing of ads\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.get_items_characteristics, self.path_links, self.df_path) for _ in range(10)]  # Adjust the number of threads as needed\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                parsed, closed = future.result()\n",
    "                while parsed + closed < counter_ads:\n",
    "                    time.sleep(60)\n",
    "                    parsed, closed = self.get_items_characteristics(self.path_links, self.df_path)\n",
    "\n",
    "    def get_items_characteristics(self, path_links, df_path):\n",
    "        print('getting chracteristics')\n",
    "        \"\"\"\n",
    "        Parse the characteristics of the ads from the given URLs.\n",
    "\n",
    "        :param path_links: Path to the file containing the URLs of the ads.\n",
    "        :param df_path: Path to the CSV file where parsed data will be stored.\n",
    "        :return: Tuple containing the number of parsed and closed ads.\n",
    "        \"\"\"\n",
    "        chrome_options = Options()\n",
    "        prefs = {\n",
    "            \"profile.managed_default_content_settings.images\": 2,  # 2 means block images\n",
    "            \"profile.default_content_setting_values.images\": 2,    # 2 means block images\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(), options=chrome_options)\n",
    "\n",
    "        parsed_data = []\n",
    "        failed_links = []\n",
    "        closed_list = []\n",
    "\n",
    "        if os.path.exists(df_path):\n",
    "            df = pd.read_csv(df_path)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=['id'])\n",
    "\n",
    "        with open(path_links, 'r') as fp:\n",
    "            urls = fp.readlines()\n",
    "        \n",
    "        random.shuffle(urls)  \n",
    "        \n",
    "        for url in tqdm(urls):\n",
    "            url = url.strip()\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            id = url.split('/doma_dachi_kottedzhi/')[1].split('?context')[0]\n",
    "\n",
    "            if id in df['id'].values or id in closed_list:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # print(id, 'started')\n",
    "                driver.get(url)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                chars = {}\n",
    "                for ultag in soup.find_all('ul', {'class': 'params-paramsList-_awNW'}):\n",
    "                    for litag in ultag.find_all('li'):\n",
    "                        l = litag.text.split(': ')\n",
    "                        chars[l[0]] = l[1].replace('\\xa0', '').replace('сот.', '').replace('м²', '').replace('км', '')\n",
    "\n",
    "                chars['address'] = soup.find(itemprop=\"address\").text\n",
    "                chars['price'] = soup.find(\"span\", itemprop=\"price\").text.replace('\\xa0', '').replace('₽', '')\n",
    "                chars['description'] = soup.find('div', {'itemprop': 'description'}).text\n",
    "                chars['seller'] = soup.find('div', {'data-marker': 'seller-info/label'}).text\n",
    "                chars['category'] = soup.find_all(itemprop=\"name\")[4].text\n",
    "\n",
    "                try:\n",
    "                    chars['lat'] = soup.find(class_='style-item-map-wrapper-ElFsX style-expanded-x335n').attrs['data-map-lat']\n",
    "                    chars['lon'] = soup.find(class_='style-item-map-wrapper-ElFsX style-expanded-x335n').attrs['data-map-lon']\n",
    "                except Exception:\n",
    "                    failed_links.append(url)\n",
    "                    continue\n",
    "\n",
    "                chars['id'] = id\n",
    "                df = pd.concat([df, pd.DataFrame([chars])], ignore_index=True)\n",
    "                df.to_csv(df_path, index=False)\n",
    "                parsed_data.append(id)\n",
    "                # print(id, 'parsed')\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    closed = soup.find_all(class_=\"closed-warning-block-_5cSD\")\n",
    "                    if len(closed) > 0:\n",
    "                        closed_list.append(id)\n",
    "                        # print(id, 'closed')\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        failed_links.append(url)\n",
    "                        time.sleep(5)\n",
    "                except Exception as e:\n",
    "                    failed_links.append(url)\n",
    "                    time.sleep(5)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        if failed_links:\n",
    "            with open('failed_links.txt', 'w') as f:\n",
    "                for link in failed_links:\n",
    "                    f.write(f'{link}\\n')\n",
    "            print('Number of failed adds: ', len(failed_links))\n",
    "            print('Failed links saved to failed_links.txt')\n",
    "\n",
    "        if closed_list:\n",
    "            with open('closed_list.txt', 'w') as f:\n",
    "                for item in closed_list:\n",
    "                    f.write(f'{item}\\n')\n",
    "            print('Number of closed adds: ', len(closed_list))\n",
    "            print('Closed list saved to closed_list.txt')\n",
    "\n",
    "        print('Number of parsed adds: ', len(parsed_data))\n",
    "        return [len(parsed_data), len(closed_list)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4098f120-70f2-4d26-bc2c-6fa9cb3083fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = AvitoParser()\n",
    "# parser.initializer(target_types = ['dom'], parse_new_links = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f89b08-e694-4abe-a9ba-9fd63319be74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvpfJrJ2vp-_"
   },
   "source": [
    "# Imports and infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RP9y2-osPyWI"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers==4.49.0\n",
    "# !pip install optuna==2.10.0\n",
    "# !pip install numpy==1.26.4 gensim==4.3.2\n",
    "# !pip install scipy==1.12.0\n",
    "# !pip install --upgrade pandas==2.2.2\n",
    "# !pip install h3\n",
    "# !pip install mlflow\n",
    "# !pip install 'protobuf<4'\n",
    "# !pip install selenium\n",
    "# !pip install natasha\n",
    "# !pip install pymystem3\n",
    "# !pip install symspellpy\n",
    "!rm -rf /content/Price-prediction-with-textual-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrwiuPmxkr_3",
    "outputId": "f13b2e5b-630d-4c3c-bcb5-c0cff07eb6e3"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/anna-k-00/Price-prediction-with-textual-data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EaKe5niAZQuS",
    "outputId": "8277e38d-0da9-4ea6-e8ee-37f270c1f447"
   },
   "outputs": [],
   "source": [
    "# Шаг 1: Проверка и настройка окружения\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Шаг 2: Клонирование/обновление репозитория\n",
    "repo_url = 'https://github.com/anna-k-00/Price-prediction-with-textual-data.git'\n",
    "repo_dir = 'Price-prediction-with-textual-data'\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone {repo_url}\n",
    "else:\n",
    "    !cd {repo_dir} && git pull\n",
    "\n",
    "# Шаг 3: Добавляем все нужные пути в sys.path\n",
    "paths_to_add = [\n",
    "    f'/content/{repo_dir}',                     # Для файлов в корне (parser_avito.py)\n",
    "    f'/content/{repo_dir}/main_methods',        # Основные модули\n",
    "    f'/content/{repo_dir}/embeddings_generation', # Генерация эмбеддингов\n",
    "    f'/content/{repo_dir}/preprocessors'        # Препроцессоры\n",
    "]\n",
    "\n",
    "for path in paths_to_add:\n",
    "    if os.path.exists(path) and path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "        print(f'Добавлен путь: {path}')\n",
    "\n",
    "# Шаг 4: Собираем список всех модулей для импорта\n",
    "all_modules = [\n",
    "    # Основные модули\n",
    "    'resource_monitor', 'ANN', 'predict', 'test_pipeline',\n",
    "\n",
    "    # Модули из embeddings_generation\n",
    "    'embeddings_generation.rubert_fine_tuning',\n",
    "    'embeddings_generation.tfidf_generator',\n",
    "    'embeddings_generation.w2v_generator',\n",
    "    'embeddings_generation.gate',\n",
    "\n",
    "    # Модули из preprocessors\n",
    "    'preprocessors.preprocessor_params_hex',\n",
    "    'preprocessors.preprocessor_text',\n",
    "\n",
    "    # Отдельные файлы в корне\n",
    "    'parser_avito'\n",
    "]\n",
    "\n",
    "# Шаг 5: Импортируем все модули\n",
    "imported_modules = {}\n",
    "failed_modules = {}\n",
    "\n",
    "for module_name in all_modules:\n",
    "    try:\n",
    "        module = importlib.import_module(module_name)\n",
    "        imported_modules[module_name] = module\n",
    "        print(f'✅ {module_name} успешно импортирован')\n",
    "    except Exception as e:\n",
    "        failed_modules[module_name] = str(e)\n",
    "        print(f'❌ Ошибка импорта {module_name}: {str(e)[:200]}')  # Обрезаем длинные сообщения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQLHgOjijQz7",
    "outputId": "f5ca17b0-e3aa-4afb-86de-f8056cd78a33"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import joblib\n",
    "import transformers\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import sys\n",
    "import platform\n",
    "import psutil\n",
    "import threading\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    HAS_NVML = True\n",
    "except Exception:\n",
    "    HAS_NVML = False\n",
    "\n",
    "from ANN import ANNRegressor\n",
    "from resource_monitor import ResourceMonitor\n",
    "from test_pipeline import PricePredictionExperiment\n",
    "from preprocessor_params_hex import DataProcessingPipeline\n",
    "\n",
    "from tfidf_generator import TfidfTransformer\n",
    "from w2v_generator import Word2VecTransformer\n",
    "from rubert_fine_tuning import RuBertTiny2Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msafR8KFe4qO",
    "outputId": "374f1faf-3a9d-4d40-b14c-6a9b5598c2e7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VTiN1erciSc"
   },
   "outputs": [],
   "source": [
    "# mkdir -p \"/content/drive/My Drive/mlflow_data_DROP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-4mqqeleoCh"
   },
   "outputs": [],
   "source": [
    "# set your ml flow directory\n",
    "\n",
    "mlflow.set_tracking_uri('file:///content/drive/My Drive/mlflow_data_DROP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoQ4vFUykAvZ"
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-g9au64OQpo"
   },
   "source": [
    "# Анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWt0lARSYLpM"
   },
   "source": [
    "т.к ml flow folder с большим количеством runs становится сложно итерировать для поиска runs и collab запрещает поднимать лолкальные сервера лдя работы с интерфейсом, в центральном пайплайне для эксперимнета мы дублировали сохранение основных файлов экспериментов (метрики, предсказания, параметры, модели, трансформеры, токенизаторы) в локальные папки архивы всех тренировок доступны в папке https://drive.google.com/drive/folders/10uxDBjledOSIg6biJpLv6WgCMVQqzesT?usp=sharing\n",
    "\n",
    "Results_pca - https://drive.google.com/drive/folders/1kFCVdfSFN3nQHjhctiebiFOmlbGzYdDW?usp=share_link - основной фолдер с результатами первых 28 экспериментов\n",
    "manual_text_features_negative - https://drive.google.com/drive/folders/1cUMsJQWtKzEUzXahFsHnDw6O1lNZAnkp?usp=sharing - фолдер с резулттатами доп экспериментов с только негативными ручнми текстовыми признаками\n",
    "manual_text_features - https://drive.google.com/drive/folders/1uEKoHcxSOnZeDUjzoudHBJ2Uk99dFW2e?usp=share_link - фолдер с резулттатами доп экспериментов со всеми ручнми текстовыми признаками\n",
    "доступы открыты и для простоты аналитики мы будем брать результаты осноных экспериментов за март оттуда\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "самый простой способ получить доступ к нашему архиву с результатами - по ссылке 'https://drive.google.com/drive/folders/10uxDBjledOSIg6biJpLv6WgCMVQqzesT?usp=sharing'\n",
    "\n",
    "через интерфейс Google Drive:\n",
    "\n",
    "Перейдите по нашей ссылке будучи авторизованными (доступы уже открыты)\n",
    "Откройте Google Drive\n",
    "Найдите папку в разделе \"Доступно мне\" (Shared with me)\n",
    "Правой кнопкой мыши → \"Добавить ярлык на Диск\" (Add shortcut to Drive)\n",
    "Выберите расположение в \"Мой диск\" → Нажмите \"Добавить\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4XR0ex4OcR7"
   },
   "source": [
    "## Ручная систематизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoSm74PRlAAG",
    "outputId": "ec827668-203f-4012-8814-8e8f3a1f614b"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "\n",
    "# def process_model_metrics(model_short, model_long):\n",
    "#     base_path = f\"/content/drive/MyDrive/price_prediction_data/Results_pca/{model_short}/metrics\"\n",
    "#     output_path = f\"/content/drive/MyDrive/price_prediction_data/Results_pca/{model_short}\"\n",
    "\n",
    "#     combinations = [\n",
    "#         \"categorical-only_none_pca_False_gate_False\",\n",
    "#         \"mixed_rubert_pca_True_gate_False\",\n",
    "#         \"mixed_w2v_pca_True_gate_False\",\n",
    "#         \"mixed_tfidf_pca_True_gate_False\",\n",
    "#         \"text-only_rubert_pca_True_gate_False\",\n",
    "#         \"text-only_w2v_pca_True_gate_False\",\n",
    "#         \"text-only_tfidf_pca_True_gate_False\"\n",
    "#     ]\n",
    "\n",
    "#     for combo in combinations:\n",
    "#         try:\n",
    "#             # 1. Загрузка данных\n",
    "#             summary_file = f\"summary_metrics_price_prediction_{model_long}_{combo}.csv\"\n",
    "#             summary_path = os.path.join(base_path, summary_file)\n",
    "#             cv_file = f\"cv_metrics_price_prediction_{model_long}_{combo}.json\"\n",
    "#             cv_path = os.path.join(base_path, cv_file)\n",
    "#             optuna_file = f\"optuna_trials_price_prediction_{model_long}_{combo}.csv\"\n",
    "#             optuna_path = os.path.join(base_path, optuna_file)\n",
    "\n",
    "#             if not all(os.path.exists(p) for p in [summary_path, cv_path, optuna_path]):\n",
    "#                 print(f\"Some files not found for {combo}\")\n",
    "#                 continue\n",
    "\n",
    "#             summary_df = pd.read_csv(summary_path)\n",
    "#             with open(cv_path, 'r') as f:\n",
    "#                 cv_data = json.load(f)\n",
    "#             optuna_df = pd.read_csv(optuna_path)\n",
    "\n",
    "#             # 2. Создаем финальный DataFrame\n",
    "#             final_df = pd.DataFrame()\n",
    "\n",
    "#             # 3. Добавляем лучшие параметры\n",
    "#             best_params = optuna_df.loc[optuna_df['value'].idxmax()].filter(regex='^params_')\n",
    "#             for param, value in best_params.items():\n",
    "#                 param_name = param.replace('params_', '').replace('__', '/')\n",
    "#                 final_df = pd.concat([final_df, pd.DataFrame({\n",
    "#                     'type': ['best_param'],\n",
    "#                     'metric': [param_name],\n",
    "#                     'value': [value],\n",
    "#                     'mean': [None], 'std': [None], 'conf_interval': [None],\n",
    "#                     **{f'fold_{i}': [None] for i in range(5)}  # 5 фолдов как в примере\n",
    "#                 })], ignore_index=True)\n",
    "\n",
    "#             # 4. Добавляем CV метрики в строгом порядке\n",
    "#             cv_metrics_order = ['r2', 'rmse', 'smape', 'medape']\n",
    "#             for scale in ['log', 'orig']:\n",
    "#                 cv_key = f\"{scale}_metrics\"\n",
    "#                 if cv_key in cv_data:\n",
    "#                     for metric in cv_metrics_order:\n",
    "#                         if metric in cv_data[cv_key]:\n",
    "#                             values = cv_data[cv_key][metric]['values']\n",
    "#                             final_df = pd.concat([final_df, pd.DataFrame({\n",
    "#                                 'type': [f'cv_{scale}'],\n",
    "#                                 'metric': [metric],\n",
    "#                                 'value': [None],\n",
    "#                                 'mean': [cv_data[cv_key][metric]['mean']],\n",
    "#                                 'std': [cv_data[cv_key][metric]['std']],\n",
    "#                                 'conf_interval': [cv_data[cv_key][metric]['conf_interval']],\n",
    "#                                 **{f'fold_{i}': [values[i] if i < len(values) else None] for i in range(5)}\n",
    "#                             })], ignore_index=True)\n",
    "\n",
    "#             # 5. Добавляем тестовые метрики в строгом порядке\n",
    "#             test_metrics_order = ['r2', 'rmse', 'smape', 'medape']\n",
    "#             for scale in ['log', 'orig']:\n",
    "#                 for metric in test_metrics_order:\n",
    "#                     test_row = summary_df[(summary_df['type'] == f'test_{scale}') &\n",
    "#                                         (summary_df['metric'] == metric)]\n",
    "#                     if not test_row.empty:\n",
    "#                         final_df = pd.concat([final_df, pd.DataFrame({\n",
    "#                             'type': [f'test_{scale}'],\n",
    "#                             'metric': [metric],\n",
    "#                             'value': [test_row['value'].values[0]],\n",
    "#                             'mean': [None], 'std': [None], 'conf_interval': [None],\n",
    "#                             **{f'fold_{i}': [None] for i in range(5)}\n",
    "#                         })], ignore_index=True)\n",
    "\n",
    "#             # 6. Формируем имя файла с учетом типа комбинации\n",
    "#             parts = combo.split('_')\n",
    "#             if parts[0] == 'categorical-only':\n",
    "#                 combo_name = f\"{parts[0]}_{parts[1]}\"\n",
    "#             elif parts[0] == 'text-only':\n",
    "#                 combo_name = f\"{parts[0]}_{parts[1]}\"\n",
    "#             elif parts[0] == 'mixed':\n",
    "#                 combo_name = f\"{parts[0]}_{parts[1]}\"  # mixed_w2v, mixed_tfidf, mixed_rubert\n",
    "#             else:\n",
    "#                 combo_name = combo  # fallback\n",
    "\n",
    "#             # 7. Сохраняем результат\n",
    "#             output_file = f\"metrics_{model_long}_{combo_name}_FINAL.csv\"\n",
    "#             output_file_path = os.path.join(output_path, output_file)\n",
    "\n",
    "#             final_df.to_csv(output_file_path, index=False)\n",
    "#             print(f\"Successfully saved: {output_file_path}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {combo} for {model_long}: {str(e)}\")\n",
    "\n",
    "# # Обработка всех моделей\n",
    "# models = [\n",
    "#     (\"RFR\", \"RandomForestRegressor\"),\n",
    "#     (\"LinearSVR\", \"LinearSVR\"),\n",
    "#     (\"ANN\", \"ANNRegressor\"),\n",
    "#     (\"XGBR\", \"XGBRegressor\")\n",
    "# ]\n",
    "\n",
    "# for model_short, model_long in models:\n",
    "#     print(f\"\\nProcessing model: {model_long}\")\n",
    "#     process_model_metrics(model_short, model_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXcYf4hakwig"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Путь к основной папке с результатами\n",
    "base_path = \"/content/drive/MyDrive/price_prediction_data/Results_pca\"\n",
    "\n",
    "# Список моделей\n",
    "models = [\"ANN\", \"LinearSVR\", \"RFR\", \"XGBR\"]\n",
    "\n",
    "# Функция для форматирования значений метрик\n",
    "def format_metric(metric, value, conf_interval=None, scale='log'):\n",
    "    # Проверяем, является ли value строкой и пытаемся преобразовать в float\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            value = float(value)\n",
    "        except ValueError:\n",
    "            return \"NA\"  # если преобразование невозможно\n",
    "\n",
    "    # Проверяем conf_interval аналогично\n",
    "    if conf_interval is not None and isinstance(conf_interval, str):\n",
    "        try:\n",
    "            conf_interval = float(conf_interval)\n",
    "        except ValueError:\n",
    "            conf_interval = None\n",
    "\n",
    "    if pd.isna(value):\n",
    "        return \"NA\"\n",
    "\n",
    "    if scale == 'log':\n",
    "        # Для log шкалы: все метрики с 3 знаками\n",
    "        if conf_interval is not None:\n",
    "            return f\"{round(value, 3)} ± {round(conf_interval, 3)}\"\n",
    "        return round(value, 3)\n",
    "    else:\n",
    "        # Для original шкалы: rmse округляем до целых, остальные до 3 знаков\n",
    "        if metric == 'rmse':\n",
    "            if conf_interval is not None:\n",
    "                return f\"{int(round(value))} ± {int(round(conf_interval))}\"\n",
    "            return int(round(value))\n",
    "        else:\n",
    "            if conf_interval is not None:\n",
    "                return f\"{round(value, 3)} ± {round(conf_interval, 3)}\"\n",
    "            return round(value, 3)\n",
    "\n",
    "# Функция для извлечения информации из имени файла\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    model = parts[1]\n",
    "\n",
    "    if \"categorical-only\" in filename:\n",
    "        data_type = \"non-textual\"\n",
    "        embedding = \"None\"\n",
    "    elif \"text-only\" in filename:\n",
    "        data_type = \"textual\"\n",
    "        embedding = parts[3]\n",
    "    elif \"mixed\" in filename:\n",
    "        data_type = \"mixed\"\n",
    "        embedding = parts[3]\n",
    "    else:\n",
    "        data_type = \"unknown\"\n",
    "        embedding = \"unknown\"\n",
    "\n",
    "    return model, data_type, embedding\n",
    "\n",
    "# Списки для хранения данных\n",
    "cv_log_data = []\n",
    "cv_orig_data = []\n",
    "test_log_data = []\n",
    "test_orig_data = []\n",
    "params_data = []\n",
    "\n",
    "# Проход по всем папкам и файлам\n",
    "for model in models:\n",
    "    model_path = os.path.join(base_path, model)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(model_path):\n",
    "        if filename.startswith(\"metrics_\") and filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(model_path, filename)\n",
    "\n",
    "            # Парсинг информации из имени файла\n",
    "            model_name, data_type, embedding = parse_filename(filename)\n",
    "\n",
    "            # Чтение CSV файла\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            # Извлечение параметров\n",
    "            params = df[df['type'] == 'best_param'][['metric', 'value']]\n",
    "            for _, row in params.iterrows():\n",
    "                params_data.append({\n",
    "                    'model': model_name,\n",
    "                    'data_type': data_type,\n",
    "                    'embedding': embedding,\n",
    "                    'param': row['metric'],\n",
    "                    'value': row['value']\n",
    "                })\n",
    "\n",
    "            # Извлечение метрик кросс-валидации (log)\n",
    "            cv_log = df[(df['type'] == 'cv_log')][['metric', 'mean', 'conf_interval']]\n",
    "            for _, row in cv_log.iterrows():\n",
    "                formatted_value = format_metric(\n",
    "                    row['metric'],\n",
    "                    row['mean'],\n",
    "                    row['conf_interval'],\n",
    "                    scale='log'\n",
    "                )\n",
    "                cv_log_data.append({\n",
    "                    'model': model_name,\n",
    "                    'data_type': data_type,\n",
    "                    'embedding': embedding,\n",
    "                    'metric': row['metric'],\n",
    "                    'value': formatted_value\n",
    "                })\n",
    "\n",
    "            # Извлечение метрик кросс-валидации (original)\n",
    "            cv_orig = df[(df['type'] == 'cv_orig')][['metric', 'mean', 'conf_interval']]\n",
    "            for _, row in cv_orig.iterrows():\n",
    "                formatted_value = format_metric(\n",
    "                    row['metric'],\n",
    "                    row['mean'],\n",
    "                    row['conf_interval'],\n",
    "                    scale='orig'\n",
    "                )\n",
    "                cv_orig_data.append({\n",
    "                    'model': model_name,\n",
    "                    'data_type': data_type,\n",
    "                    'embedding': embedding,\n",
    "                    'metric': row['metric'],\n",
    "                    'value': formatted_value\n",
    "                })\n",
    "\n",
    "            # Извлечение метрик теста (log)\n",
    "            test_log = df[(df['type'] == 'test_log')][['metric', 'value']]\n",
    "            for _, row in test_log.iterrows():\n",
    "                formatted_value = format_metric(\n",
    "                    row['metric'],\n",
    "                    row['value'],\n",
    "                    scale='log'\n",
    "                )\n",
    "                test_log_data.append({\n",
    "                    'model': model_name,\n",
    "                    'data_type': data_type,\n",
    "                    'embedding': embedding,\n",
    "                    'metric': row['metric'],\n",
    "                    'value': formatted_value\n",
    "                })\n",
    "\n",
    "            # Извлечение метрик теста (original)\n",
    "            test_orig = df[(df['type'] == 'test_orig')][['metric', 'value']]\n",
    "            for _, row in test_orig.iterrows():\n",
    "                formatted_value = format_metric(\n",
    "                    row['metric'],\n",
    "                    row['value'],\n",
    "                    scale='orig'\n",
    "                )\n",
    "                test_orig_data.append({\n",
    "                    'model': model_name,\n",
    "                    'data_type': data_type,\n",
    "                    'embedding': embedding,\n",
    "                    'metric': row['metric'],\n",
    "                    'value': formatted_value\n",
    "                })\n",
    "\n",
    "# Создание DataFrame для каждой таблицы\n",
    "df_cv_log = pd.DataFrame(cv_log_data)\n",
    "df_cv_orig = pd.DataFrame(cv_orig_data)\n",
    "df_test_log = pd.DataFrame(test_log_data)\n",
    "df_test_orig = pd.DataFrame(test_orig_data)\n",
    "df_params = pd.DataFrame(params_data)\n",
    "\n",
    "# Преобразование таблиц в более удобный вид (pivot)\n",
    "def create_pivot_table(df, value_col='value'):\n",
    "    return df.pivot_table(\n",
    "        index=['model', 'data_type', 'embedding'],\n",
    "        columns='metric',\n",
    "        values=value_col,\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "\n",
    "# Создание итоговых таблиц\n",
    "final_cv_log = create_pivot_table(df_cv_log)\n",
    "final_cv_orig = create_pivot_table(df_cv_orig)\n",
    "final_test_log = create_pivot_table(df_test_log)\n",
    "final_test_orig = create_pivot_table(df_test_orig)\n",
    "\n",
    "# Для параметров немного другая структура\n",
    "final_params = df_params.pivot_table(\n",
    "    index=['model', 'data_type', 'embedding', 'param'],\n",
    "    values='value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Переименование столбцов для лучшей читаемости\n",
    "metric_rename = {\n",
    "    'r2': 'R²',\n",
    "    'rmse': 'RMSE',\n",
    "    'smape': 'SMAPE'\n",
    "}\n",
    "\n",
    "for df in [final_cv_log, final_cv_orig, final_test_log, final_test_orig]:\n",
    "    df.rename(columns=metric_rename, inplace=True)\n",
    "\n",
    "# Сохранение таблиц в CSV файлы (опционально)\n",
    "final_cv_log.to_csv('cross_validation_log_scale.csv', index=False)\n",
    "final_cv_orig.to_csv('cross_validation_original_scale.csv', index=False)\n",
    "final_test_log.to_csv('test_log_scale.csv', index=False)\n",
    "final_test_orig.to_csv('test_original_scale.csv', index=False)\n",
    "final_params.to_csv('best_params.csv', index=False)\n",
    "\n",
    "# # Вывод таблиц для проверки\n",
    "# print(\"Cross Validation (Log Scale):\")\n",
    "# print(final_cv_log)\n",
    "# print(\"\\nCross Validation (Original Scale):\")\n",
    "# print(final_cv_orig)\n",
    "# print(\"\\nTest Metrics (Log Scale):\")\n",
    "# print(final_test_log)\n",
    "# print(\"\\nTest Metrics (Original Scale):\")\n",
    "# print(final_test_orig)\n",
    "# print(\"\\nBest Parameters:\")\n",
    "# print(final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TX9uGKmskwnq"
   },
   "outputs": [],
   "source": [
    "final_params['param'] = final_params['param'].str.split('/').str[-1]\n",
    "\n",
    "# Save to CSV without index\n",
    "final_params.to_csv('best_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhliwNzLnMIQ"
   },
   "outputs": [],
   "source": [
    "# !pip install scikit_posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQnCsmPbQmTu",
    "outputId": "6228f2fb-fac3-47a3-9349-f31499a4004f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = '/content/drive/MyDrive/price_prediction_data/Results_pca/'\n",
    "\n",
    "# Список для хранения найденных файлов\n",
    "prediction_files = []\n",
    "\n",
    "# Рекурсивный обход всех подпапок\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    # Проверяем, является ли текущая папка папкой 'metrics'\n",
    "    if os.path.basename(root) == 'metrics':\n",
    "        # Проверяем все файлы в этой папке\n",
    "        for file in files:\n",
    "            # Если в названии файла есть 'predictions' (регистронезависимо)\n",
    "            if 'predictions' in file.lower():\n",
    "                full_path = os.path.join(root, file)\n",
    "                prediction_files.append(full_path)\n",
    "\n",
    "# Выводим результаты\n",
    "for file_path in prediction_files:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nc8eUk6x3gaP",
    "outputId": "942f0dcf-7501-4f22-a604-0ba7c40c9c71"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = '/content/drive/MyDrive/price_prediction_data/Results_pca/'\n",
    "\n",
    "# Список для хранения найденных файлов\n",
    "prediction_files = []\n",
    "\n",
    "# Рекурсивный обход всех подпапок\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "  if os.path.basename(root) != 'metrics':\n",
    "    for file in files:\n",
    "      if 'FINAL' in file:\n",
    "        full_path = os.path.join(root, file)\n",
    "        prediction_files.append(full_path)\n",
    "    # # Проверяем, является ли текущая папка папкой 'metrics'\n",
    "    #   for file in files:\n",
    "    #     if 'Final' in file:\n",
    "    #       # Если в названии файла есть 'predictions' (регистронезависимо)\n",
    "    #         full_path = os.path.join(root, file)\n",
    "    #         prediction_files.append(full_path)\n",
    "\n",
    "# Выводим результаты\n",
    "for file_path in prediction_files:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFyXyKS45gJi",
    "outputId": "221a4a8e-c9e5-4ff4-e56d-22d968176120"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============ [0] Словари нормализации и базовые функции ============\n",
    "MODEL_MAP = {\n",
    "    \"ANNRegressor\": \"ANN\",\n",
    "    \"RandomForestRegressor\": \"RFR\",\n",
    "    \"LinearSVR\": \"LinearSVR\",\n",
    "    \"XGBRegressor\": \"XGBoost\"\n",
    "}\n",
    "MODEL_ORDER = {\"XGBoost\": 0, \"RFR\": 1, \"ANN\": 2, \"LinearSVR\": 3}\n",
    "DATASET_MAP = {\n",
    "    \"categorical-only\": \"non-textual\",\n",
    "    \"mixed\": \"mixed\",\n",
    "    \"text-only\": \"text-only\"\n",
    "}\n",
    "DATASET_ORDER = {\"non-textual\": 0, \"mixed\": 1, \"text-only\": 2}\n",
    "EMBEDDING_MAP = {\n",
    "    \"rubert\": \"rubert\",\n",
    "    \"w2v\": \"w2v\",\n",
    "    \"tfidf\": \"tfidf\",\n",
    "    \"none\": \"none\"\n",
    "}\n",
    "EMBEDDING_ORDER = {\"none\": 0, \"rubert\": 1, \"w2v\": 2, \"tfidf\": 3}\n",
    "\n",
    "def map_name(raw, mapping):\n",
    "    return mapping.get(raw, raw)\n",
    "\n",
    "def format_score(x, rmse_millions=False):\n",
    "    if pd.isnull(x): return \"\"\n",
    "    if isinstance(x, float) or isinstance(x, np.floating):\n",
    "        if rmse_millions:\n",
    "            return f'{x/1e6:.3f}'\n",
    "        if abs(x) > 10000:\n",
    "            return f'{x:,.0f}'.replace(',', ' ')\n",
    "        return f'{x:.4f}'\n",
    "    if isinstance(x, int):\n",
    "        return f'{x:,}'.replace(',', ' ')\n",
    "    return x\n",
    "\n",
    "def pretty_ci(mean, low, high, rmse_millions=False, precision=3):\n",
    "    mean_val = float(mean)\n",
    "    delta = (float(high) - float(low))/2\n",
    "    if rmse_millions:\n",
    "        return f'{mean_val/1e6:.{precision}f} ± {delta/1e6:.{precision}f}'\n",
    "    if abs(mean_val) > 10000 or abs(delta) > 10000:\n",
    "        return f'{int(mean_val):,} ± {int(delta):,}'.replace(',', ' ')\n",
    "    return f'{mean_val:.4f} ± {delta:.4f}'\n",
    "\n",
    "def sort_df(df):\n",
    "    if \"model\" in df.columns:\n",
    "        df['model_order'] = df['model'].map(MODEL_ORDER)\n",
    "    if \"data_type\" in df.columns:\n",
    "        df['dataset_order'] = df['data_type'].map(DATASET_ORDER)\n",
    "    elif \"dataset\" in df.columns:\n",
    "        df['dataset_order'] = df['dataset'].map(DATASET_ORDER)\n",
    "    if \"embedding\" in df.columns:\n",
    "        df['embedding_order'] = df['embedding'].map(EMBEDDING_ORDER)\n",
    "    sort_cols = []\n",
    "    for col in [\"model_order\", \"dataset_order\", \"embedding_order\"]:\n",
    "        if col in df.columns:\n",
    "            sort_cols.append(col)\n",
    "    if sort_cols:\n",
    "        df = df.sort_values(sort_cols)\n",
    "    for col in [\"model_order\", \"dataset_order\", \"embedding_order\"]:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "    return df\n",
    "\n",
    "# ============ [1] Получение финальной сводной таблицы метрик CV для 1.1 ============\n",
    "def format_metric(metric, value, conf_interval=None, scale='log'):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            value = float(value)\n",
    "        except ValueError:\n",
    "            return \"NA\"\n",
    "    if conf_interval is not None and isinstance(conf_interval, str):\n",
    "        try:\n",
    "            conf_interval = float(conf_interval)\n",
    "        except ValueError:\n",
    "            conf_interval = None\n",
    "    if pd.isna(value):\n",
    "        return \"NA\"\n",
    "    if scale == 'log':\n",
    "        if conf_interval is not None:\n",
    "            return f\"{round(value, 3)} ± {round(conf_interval, 3)}\"\n",
    "        return round(value, 3)\n",
    "    else:\n",
    "        if metric == 'rmse':\n",
    "            if conf_interval is not None:\n",
    "                return f\"{int(round(value))} ± {int(round(conf_interval))}\"\n",
    "            return int(round(value))\n",
    "        else:\n",
    "            if conf_interval is not None:\n",
    "                return f\"{round(value, 3)} ± {round(conf_interval, 3)}\"\n",
    "            return round(value, 3)\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    model = parts[1]\n",
    "    if \"categorical-only\" in filename:\n",
    "        data_type = \"non-textual\"\n",
    "        embedding = \"none\"\n",
    "    elif \"text-only\" in filename:\n",
    "        data_type = \"text-only\"\n",
    "        embedding = parts[3]\n",
    "    elif \"mixed\" in filename:\n",
    "        data_type = \"mixed\"\n",
    "        embedding = parts[3]\n",
    "    else:\n",
    "        data_type = \"unknown\"\n",
    "        embedding = \"unknown\"\n",
    "    return map_name(model, MODEL_MAP), data_type, embedding\n",
    "\n",
    "base_path = \"/content/drive/MyDrive/price_prediction_data/Results_pca\"\n",
    "models = [\"ANN\", \"LinearSVR\", \"RFR\", \"XGBR\"]\n",
    "cv_orig_data = []\n",
    "for model in models:\n",
    "    model_path = os.path.join(base_path, model)\n",
    "    if not os.path.exists(model_path):\n",
    "        continue\n",
    "    for filename in os.listdir(model_path):\n",
    "        if filename.startswith(\"metrics_\") and filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(model_path, filename)\n",
    "            model_name, data_type, embedding = parse_filename(filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            cv_orig = df[df['type'] == 'cv_orig'][['metric', 'mean', 'conf_interval']]\n",
    "            for _, row in cv_orig.iterrows():\n",
    "                formatted_value = format_metric(\n",
    "                    row['metric'],\n",
    "                    row['mean'],\n",
    "                    row['conf_interval'],\n",
    "                    scale='orig'\n",
    "                )\n",
    "                cv_orig_data.append({\n",
    "                    'model': model_name,\n",
    "                    'data_type': data_type,\n",
    "                    'embedding': embedding,\n",
    "                    'metric': row['metric'],\n",
    "                    'value': formatted_value\n",
    "                })\n",
    "df_cv_orig = pd.DataFrame(cv_orig_data)\n",
    "def create_pivot_table(df, value_col='value'):\n",
    "    return df.pivot_table(\n",
    "        index=['model', 'data_type', 'embedding'],\n",
    "        columns='metric',\n",
    "        values=value_col,\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "metric_rename = {\n",
    "    'r2': 'R²',\n",
    "    'rmse': 'RMSE',\n",
    "    'smape': 'SMAPE'\n",
    "}\n",
    "final_cv_orig = create_pivot_table(df_cv_orig)\n",
    "final_cv_orig.rename(columns=metric_rename, inplace=True)\n",
    "final_cv_orig = sort_df(final_cv_orig)\n",
    "\n",
    "# ============ [2] Загрузка тестовых предсказаний ============\n",
    "print(\"\\n=== [2] Загрузка тестовых предсказаний ===\")\n",
    "pred_files = glob.glob('/content/drive/MyDrive/price_prediction_data/Results_pca/*/metrics/predictions_test_price_prediction_*.csv')\n",
    "preds_list = []\n",
    "for pf in tqdm(pred_files):\n",
    "    try:\n",
    "        pred_df = pd.read_csv(pf)\n",
    "        fname = os.path.basename(pf)\n",
    "        arr = fname.split('_')\n",
    "        model = map_name(arr[4], MODEL_MAP)\n",
    "        dataset = map_name(arr[5], DATASET_MAP)\n",
    "        embedding = map_name(arr[6], EMBEDDING_MAP)\n",
    "        preds_list.append({\n",
    "            'model': model,\n",
    "            'dataset': dataset,\n",
    "            'embedding': embedding,\n",
    "            'true_price': pred_df['true_price'].values,\n",
    "            'predicted_price': pred_df['predicted_price'].values,\n",
    "        })\n",
    "        print(f\"  Загружен: {model} | {dataset} | {embedding} | {len(pred_df)} точек\")\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {pf}: {e}')\n",
    "print(f\"Итого успешно загружено {len(preds_list)} файлов с предсказаниями.\\n\")\n",
    "\n",
    "# ============ [3] Holdout сводка ============\n",
    "def bootstrap_point_metric(true, pred, n_bootstrap=1000):\n",
    "    stats = []\n",
    "    n = len(true)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        r2 = 1 - np.sum((true[idx]-pred[idx])**2) / np.sum((true[idx] - np.mean(true[idx]))**2)\n",
    "        rmse = np.sqrt(np.mean((true[idx]-pred[idx])**2))\n",
    "        medape = np.median(np.abs((true[idx] - pred[idx]) / true[idx])) * 100\n",
    "        stats.append((r2, rmse, medape))\n",
    "    stats = np.array(stats)\n",
    "    res = {\n",
    "        'R²holdout': np.mean(stats[:,0]), 'R²holdout_low': np.percentile(stats[:,0],2.5), 'R²holdout_high': np.percentile(stats[:,0],97.5),\n",
    "        'RMSEholdout': np.mean(stats[:,1]), 'RMSEholdout_low': np.percentile(stats[:,1],2.5), 'RMSEholdout_high': np.percentile(stats[:,1],97.5),\n",
    "        'MedAPEholdout': np.mean(stats[:,2]), 'MedAPEholdout_low': np.percentile(stats[:,2],2.5), 'MedAPEholdout_high': np.percentile(stats[:,2],97.5)\n",
    "    }\n",
    "    return res\n",
    "\n",
    "def holdout_metrics_table(preds_list, metrics_names=None):\n",
    "    out = []\n",
    "    for d in preds_list:\n",
    "        stats = bootstrap_point_metric(np.array(d['true_price']), np.array(d['predicted_price']))\n",
    "        out.append({\n",
    "            'model': d['model'],\n",
    "            'dataset': d['dataset'],\n",
    "            'embedding': d['embedding'],\n",
    "            **stats\n",
    "        })\n",
    "    df = pd.DataFrame(out)\n",
    "    if metrics_names is None:\n",
    "        metrics_names = {\n",
    "            'R²holdout': r'$R^2_{\\text{holdout}}$',\n",
    "            'RMSEholdout': r'$\\mathrm{RMSE}_{\\text{holdout}},$ млн',\n",
    "            'MedAPEholdout': r'$\\mathrm{MedAPE}_{\\text{holdout}},$ %'\n",
    "        }\n",
    "    df['R²holdout ±CI'] = df.apply(lambda r: pretty_ci(r['R²holdout'], r['R²holdout_low'], r['R²holdout_high']), axis=1)\n",
    "    df['RMSEholdout ±CI, млн'] = df.apply(lambda r: pretty_ci(r['RMSEholdout'], r['RMSEholdout_low'], r['RMSEholdout_high'], rmse_millions=True), axis=1)\n",
    "    df['MedAPEholdout ±CI, %'] = df.apply(lambda r: pretty_ci(r['MedAPEholdout'], r['MedAPEholdout_low'], r['MedAPEholdout_high']), axis=1)\n",
    "    return sort_df(df[['model','dataset','embedding','R²holdout ±CI','RMSEholdout ±CI, млн','MedAPEholdout ±CI, %']])\n",
    "\n",
    "test_metrics_df_vis = holdout_metrics_table(preds_list)\n",
    "\n",
    "# ============ [4] Бутстрап и сегменты ============\n",
    "def get_segments(y):\n",
    "    quant = np.quantile(y, 0.85)\n",
    "    idx_low = np.where(y <= quant)[0]\n",
    "    idx_high = np.where(y > quant)[0]\n",
    "    return idx_low, idx_high\n",
    "\n",
    "def build_segmented_predslists(preds_list):\n",
    "    seg_preds = {'low': [], 'high': []}\n",
    "    for d in preds_list:\n",
    "        y = np.array(d['true_price'])\n",
    "        y_pred = np.array(d['predicted_price'])\n",
    "        idx_low, idx_high = get_segments(y)\n",
    "        seg_preds['low'].append({**d, 'true_price': y[idx_low], 'predicted_price': y_pred[idx_low]})\n",
    "        seg_preds['high'].append({**d, 'true_price': y[idx_high], 'predicted_price': y_pred[idx_high]})\n",
    "    return seg_preds\n",
    "\n",
    "seg_preds = build_segmented_predslists(preds_list)\n",
    "test_metrics_df_vis_low = holdout_metrics_table(seg_preds['low'])\n",
    "test_metrics_df_vis_high = holdout_metrics_table(seg_preds['high'])\n",
    "\n",
    "# ============ [5] Остальные CV-таблицы ============\n",
    "def friedman_posthoc_report(data, metric, group_vars=['model', 'dataset']):\n",
    "    best_by_group = []\n",
    "    for key, subdf in data[data['metric'] == metric].groupby(group_vars):\n",
    "        try:\n",
    "            piv = subdf.set_index('embedding')[[f'fold_{i}' for i in range(5)]]\n",
    "            means = piv.mean(axis=1)\n",
    "            n_var = len(piv)\n",
    "            best = means.idxmax() if metric == 'r2' else means.idxmin()\n",
    "            best_score = means[best]\n",
    "            second, second_score = (None, None)\n",
    "            p_value = None\n",
    "            stat_significant = None\n",
    "            comment = \"\"\n",
    "            if n_var > 1:\n",
    "                second = means.drop(best).idxmax() if metric == 'r2' else means.drop(best).idxmin()\n",
    "                second_score = means[second]\n",
    "            if n_var == 1:\n",
    "                comment = \"Only one embedding.\"\n",
    "            elif n_var == 2:\n",
    "                v1, v2 = piv.index\n",
    "                s, p_w = wilcoxon(piv.loc[v1], piv.loc[v2])\n",
    "                p_value = p_w\n",
    "                stat_significant = p_w < 0.05\n",
    "                comment = (f\"Best embedding: {best}, score={format_score(best_score)}\" +\n",
    "                           (f\" (statistically significant, p={p_value:.4f})\" if stat_significant else\n",
    "                            f\"; second best: {second}, score={format_score(second_score)} (NOT statistically significant, p={p_value:.4f})\"))\n",
    "            else:\n",
    "                stat, p = friedmanchisquare(*[row.values for _, row in piv.iterrows()])\n",
    "                p_value = p\n",
    "                stat_significant = p < 0.05\n",
    "                comment = (f\"Best embedding: {best}, score={format_score(best_score)}\" +\n",
    "                           (f\" (statistically significant, p={p_value:.4f})\" if stat_significant else\n",
    "                            f\"; second best: {second}, score={format_score(second_score)} (NOT statistically significant, p={p_value:.4f})\"))\n",
    "            best_by_group.append({\n",
    "                **dict(zip(group_vars, key)),\n",
    "                'metric': metric,\n",
    "                'best_embedding': best,\n",
    "                'best_score': format_score(best_score),\n",
    "                'second_embedding': second,\n",
    "                'second_score': format_score(second_score) if second_score is not None else None,\n",
    "                'stat_significant': stat_significant,\n",
    "                'p_value': format_score(p_value) if p_value is not None else None,\n",
    "                'comment': comment\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f'Error for {key}: {e}')\n",
    "    return pd.DataFrame(best_by_group)\n",
    "\n",
    "def best_dataset_by_model_report(best_embeds, df_metrics):\n",
    "    out = []\n",
    "    for metric in ['r2', 'rmse', 'medape']:\n",
    "        for model, sdf in best_embeds[best_embeds['metric']==metric].groupby('model'):\n",
    "            variants = []\n",
    "            for _, row in sdf.iterrows():\n",
    "                ds = row['dataset']\n",
    "                embed = row['best_embedding']\n",
    "                v = df_metrics[(df_metrics['model']==model)&\n",
    "                               (df_metrics['dataset']==ds)&\n",
    "                               (df_metrics['embedding']==embed)&\n",
    "                               (df_metrics['metric']==metric)]\n",
    "                if not v.empty:\n",
    "                    vals = v[[f'fold_{i}' for i in range(5)]].values.flatten()\n",
    "                    variants.append((ds, vals))\n",
    "            piv = pd.DataFrame({ds: vals for ds, vals in variants}).T\n",
    "            means = piv.mean(axis=1)\n",
    "            n_var = len(piv)\n",
    "            best = means.idxmax() if metric == 'r2' else means.idxmin()\n",
    "            best_score = means[best]\n",
    "            second, second_score = (None, None)\n",
    "            p_value = None\n",
    "            stat_significant = None\n",
    "            comment = \"\"\n",
    "            if n_var > 1:\n",
    "                second = means.drop(best).idxmax() if metric == 'r2' else means.drop(best).idxmin()\n",
    "                second_score = means[second]\n",
    "            if n_var == 1:\n",
    "                comment = \"Only one dataset.\"\n",
    "            elif n_var == 2:\n",
    "                v1, v2 = piv.index\n",
    "                s, p_w = wilcoxon(piv.loc[v1], piv.loc[v2])\n",
    "                p_value = p_w\n",
    "                stat_significant = p_w < 0.05\n",
    "                comment = (f\"Best dataset: {best}, score={format_score(best_score)}\" +\n",
    "                           (f\" (statistically significant, p={p_value:.4f})\" if stat_significant else\n",
    "                            f\"; second best: {second}, score={format_score(second_score)} (NOT statistically significant, p={p_value:.4f})\"))\n",
    "            else:\n",
    "                stat, p = friedmanchisquare(*[row.values for _, row in piv.iterrows()])\n",
    "                p_value = p\n",
    "                stat_significant = p < 0.05\n",
    "                comment = (f\"Best dataset: {best}, score={format_score(best_score)}\" +\n",
    "                           (f\" (statistically significant, p={p_value:.4f})\" if stat_significant else\n",
    "                            f\"; second best: {second}, score={format_score(second_score)} (NOT statistically significant, p={p_value:.4f})\"))\n",
    "            out.append({'model': model, 'metric': metric,\n",
    "                        'best_dataset': best, 'best_score': format_score(best_score),\n",
    "                        'second_dataset': second, 'second_score': format_score(second_score) if second_score is not None else None,\n",
    "                        'stat_significant': stat_significant, 'p_value': format_score(p_value) if p_value is not None else None,\n",
    "                        'comment': comment})\n",
    "    df = pd.DataFrame(out)\n",
    "    return sort_df(df)\n",
    "\n",
    "def best_model_by_metric_report(best_datasets, best_embeds, df_metrics):\n",
    "    out = []\n",
    "    for metric in ['r2', 'rmse', 'medape']:\n",
    "        variants = []\n",
    "        for _, row in best_datasets[best_datasets['metric']==metric].iterrows():\n",
    "            model = row['model']\n",
    "            ds = row['best_dataset']\n",
    "            embed = best_embeds[(best_embeds['model']==model)&(best_embeds['dataset']==ds)&(best_embeds['metric']==metric)]['best_embedding'].values[0]\n",
    "            v = df_metrics[(df_metrics['model']==model)&(df_metrics['dataset']==ds)&(df_metrics['embedding']==embed)&(df_metrics['metric']==metric)]\n",
    "            if not v.empty:\n",
    "                vals = v[[f'fold_{i}' for i in range(5)]].values.flatten()\n",
    "                variants.append((model, vals))\n",
    "        piv = pd.DataFrame({model: vals for model, vals in variants}).T\n",
    "        means = piv.mean(axis=1)\n",
    "        n_var = len(piv)\n",
    "        best = means.idxmax() if metric == 'r2' else means.idxmin()\n",
    "        best_score = means[best]\n",
    "        second, second_score = (None, None)\n",
    "        p_value = None\n",
    "        stat_significant = None\n",
    "        comment = \"\"\n",
    "        if n_var > 1:\n",
    "            second = means.drop(best).idxmax() if metric == 'r2' else means.drop(best).idxmin()\n",
    "            second_score = means[second]\n",
    "        if n_var == 1:\n",
    "            comment = \"Only one model.\"\n",
    "        elif n_var == 2:\n",
    "            v1, v2 = piv.index\n",
    "            s, p_w = wilcoxon(piv.loc[v1], piv.loc[v2])\n",
    "            p_value = p_w\n",
    "            stat_significant = p_w < 0.05\n",
    "            comment = (f\"Best model: {best}, score={format_score(best_score)}\" +\n",
    "                       (f\" (statistically significant, p={p_value:.4f})\" if stat_significant else\n",
    "                        f\"; second best: {second}, score={format_score(second_score)} (NOT statistically significant, p={p_value:.4f})\"))\n",
    "        else:\n",
    "            stat, p = friedmanchisquare(*[row.values for _, row in piv.iterrows()])\n",
    "            p_value = p\n",
    "            stat_significant = p < 0.05\n",
    "            comment = (f\"Best model: {best}, score={format_score(best_score)}\" +\n",
    "                       (f\" (statistically significant, p={p_value:.4f})\" if stat_significant else\n",
    "                        f\"; second best: {second}, score={format_score(second_score)} (NOT statistically significant, p={p_value:.4f})\"))\n",
    "        out.append({'metric': metric,\n",
    "                    'best_model': best, 'best_score': format_score(best_score),\n",
    "                    'second_model': second, 'second_score': format_score(second_score) if second_score is not None else None,\n",
    "                    'stat_significant': stat_significant, 'p_value': format_score(p_value) if p_value is not None else None,\n",
    "                    'comment': comment})\n",
    "    return sort_df(pd.DataFrame(out))\n",
    "\n",
    "# (df_metrics нужен для CV-таблиц, предположим что вы загружаете его где-то выше — не трогаем эту логику)\n",
    "\n",
    "# ============ [6] Bootstrap по всему holdout и сегментам ============\n",
    "def calc_metrics(true, pred):\n",
    "    r2 = 1 - np.sum((true - pred)**2) / np.sum((true - np.mean(true))**2)\n",
    "    rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "    medape = np.median(np.abs((true - pred) / true)) * 100\n",
    "    return {'r2': r2, 'rmse': rmse, 'medape': medape}\n",
    "\n",
    "def bootstrap_compare_preds(arr1, arr2, metric, n_bootstrap=1000, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    n = min(len(arr1['true']), len(arr2['true']))\n",
    "    indices = np.arange(n)\n",
    "    diffs = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(indices, size=n, replace=True)\n",
    "        m1 = calc_metrics(arr1['true'][idx], arr1['pred'][idx])[metric]\n",
    "        m2 = calc_metrics(arr2['true'][idx], arr2['pred'][idx])[metric]\n",
    "        diff = m1 - m2 if metric == 'r2' else m2 - m1\n",
    "        diffs.append(diff)\n",
    "    diffs = np.array(diffs)\n",
    "    ci_low, ci_high = np.percentile(diffs, [2.5, 97.5])\n",
    "    return np.mean(diffs), ci_low, ci_high\n",
    "\n",
    "def bootstrap_embeddings(preds_list, metric, segment_name=None):\n",
    "    rows = []\n",
    "    df = pd.DataFrame(preds_list)\n",
    "    for model in sorted(df['model'].unique(), key=lambda m: MODEL_ORDER.get(m, 100)):\n",
    "        for dataset in sorted(df['dataset'].unique(), key=lambda d: DATASET_ORDER.get(d, 100)):\n",
    "            group = df[(df['model']==model) & (df['dataset']==dataset)]\n",
    "            if group.shape[0] < 2:\n",
    "                continue\n",
    "            for_emb = {}\n",
    "            for _, row in group.iterrows():\n",
    "                for_emb[row['embedding']] = {'true': row['true_price'], 'pred': row['predicted_price']}\n",
    "            emb_avail = list(for_emb.keys())\n",
    "            if len(emb_avail) < 2: continue\n",
    "            means = {emb: calc_metrics(for_emb[emb]['true'], for_emb[emb]['pred'])[metric] for emb in emb_avail}\n",
    "            sorted_emb = sorted(means, key=lambda x: means[x], reverse=(metric=='r2'))\n",
    "            best = sorted_emb[0]\n",
    "            second = sorted_emb[1]\n",
    "            best_score = means[best]\n",
    "            second_score = means[second]\n",
    "            mean_diff, ci_low, ci_high = bootstrap_compare_preds(for_emb[best], for_emb[second], metric)\n",
    "            row_result = {\n",
    "                'model': model, 'dataset': dataset, 'metric': metric,\n",
    "                'best_embedding': best, 'best_score': format_score(best_score, rmse_millions=(metric=='rmse')),\n",
    "                'second_embedding': second, 'second_score': format_score(second_score, rmse_millions=(metric=='rmse')),\n",
    "                'diff': pretty_ci(mean_diff, ci_low, ci_high, rmse_millions=(metric=='rmse')),\n",
    "            }\n",
    "            if segment_name: row_result['segment'] = segment_name\n",
    "            rows.append(row_result)\n",
    "    return sort_df(pd.DataFrame(rows))\n",
    "\n",
    "def bootstrap_datasets(preds_list, metric, segment_name=None):\n",
    "    rows = []\n",
    "    df = pd.DataFrame(preds_list)\n",
    "    for model in sorted(df['model'].unique(), key=lambda m: MODEL_ORDER.get(m, 100)):\n",
    "        group = df[df['model']==model]\n",
    "        ds_map = {}\n",
    "        for dataset in sorted(df['dataset'].unique(), key=lambda d: DATASET_ORDER.get(d, 100)):\n",
    "            cur = group[group['dataset']==dataset]\n",
    "            if cur.empty: continue\n",
    "            best_emb = None\n",
    "            best_val = None\n",
    "            for emb in cur['embedding'].unique():\n",
    "                vals = calc_metrics(cur[cur['embedding'] == emb].iloc[0]['true_price'],\n",
    "                                   cur[cur['embedding'] == emb].iloc[0]['predicted_price'])[metric]\n",
    "                if (best_val is None) or ((metric == 'r2' and vals > best_val) or (metric != 'r2' and vals < best_val)):\n",
    "                    best_val = vals\n",
    "                    best_emb = emb\n",
    "            if best_emb is not None:\n",
    "                row = cur[cur['embedding'] == best_emb].iloc[0]\n",
    "                ds_map[dataset] = {'true': row['true_price'], 'pred': row['predicted_price']}\n",
    "        ds_avail = list(ds_map.keys())\n",
    "        if len(ds_avail) < 2: continue\n",
    "        means = {ds: calc_metrics(ds_map[ds]['true'], ds_map[ds]['pred'])[metric] for ds in ds_avail}\n",
    "        sorted_ds = sorted(means, key=lambda x: means[x], reverse=(metric=='r2'))\n",
    "        best = sorted_ds[0]\n",
    "        second = sorted_ds[1]\n",
    "        best_score = means[best]\n",
    "        second_score = means[second]\n",
    "        mean_diff, ci_low, ci_high = bootstrap_compare_preds(ds_map[best], ds_map[second], metric)\n",
    "        row_result = {\n",
    "            'model': model, 'metric': metric,\n",
    "            'best_dataset': best, 'best_score': format_score(best_score, rmse_millions=(metric=='rmse')),\n",
    "            'second_dataset': second, 'second_score': format_score(second_score, rmse_millions=(metric=='rmse')),\n",
    "            'diff': pretty_ci(mean_diff, ci_low, ci_high, rmse_millions=(metric=='rmse')),\n",
    "        }\n",
    "        if segment_name: row_result['segment'] = segment_name\n",
    "        rows.append(row_result)\n",
    "    return sort_df(pd.DataFrame(rows))\n",
    "\n",
    "def bootstrap_models(preds_list, metric, segment_name=None):\n",
    "    rows = []\n",
    "    df = pd.DataFrame(preds_list)\n",
    "    best_rows = []\n",
    "    for model in sorted(df['model'].unique(), key=lambda m: MODEL_ORDER.get(m, 100)):\n",
    "        group = df[df['model']==model]\n",
    "        best_metric = None\n",
    "        best_row = None\n",
    "        for _, row in group.iterrows():\n",
    "            val = calc_metrics(row['true_price'], row['predicted_price'])[metric]\n",
    "            if (best_metric is None) or ((metric=='r2' and val > best_metric) or (metric!='r2' and val < best_metric)):\n",
    "                best_metric = val\n",
    "                best_row = row\n",
    "        if best_row is not None:\n",
    "            best_rows.append(best_row)\n",
    "    if len(best_rows) < 2: return pd.DataFrame()\n",
    "    mod_map = {row['model']: {'true': row['true_price'], 'pred': row['predicted_price']} for row in best_rows}\n",
    "    means = {m: calc_metrics(mod_map[m]['true'], mod_map[m]['pred'])[metric] for m in mod_map}\n",
    "    sorted_m = sorted(means, key=lambda x: means[x], reverse=(metric=='r2'))\n",
    "    best = sorted_m[0]\n",
    "    second = sorted_m[1]\n",
    "    best_score = means[best]\n",
    "    second_score = means[second]\n",
    "    mean_diff, ci_low, ci_high = bootstrap_compare_preds(mod_map[best], mod_map[second], metric)\n",
    "    row_result = {\n",
    "        'metric': metric,\n",
    "        'best_model': best, 'best_score': format_score(best_score, rmse_millions=(metric=='rmse')),\n",
    "        'second_model': second, 'second_score': format_score(second_score, rmse_millions=(metric=='rmse')),\n",
    "        'diff': pretty_ci(mean_diff, ci_low, ci_high, rmse_millions=(metric=='rmse')),\n",
    "    }\n",
    "    if segment_name: row_result['segment'] = segment_name\n",
    "    rows.append(row_result)\n",
    "    return sort_df(pd.DataFrame(rows))\n",
    "\n",
    "# Все bootstrap\n",
    "all_bootstrap_emb, all_bootstrap_ds, all_bootstrap_mod = [], [], []\n",
    "for metric in ['r2', 'rmse', 'medape']:\n",
    "    all_bootstrap_emb.append(bootstrap_embeddings(preds_list, metric))\n",
    "    all_bootstrap_ds.append(bootstrap_datasets(preds_list, metric))\n",
    "    all_bootstrap_mod.append(bootstrap_models(preds_list, metric))\n",
    "bootstrap_emb = pd.concat(all_bootstrap_emb, ignore_index=True)\n",
    "bootstrap_ds = pd.concat(all_bootstrap_ds, ignore_index=True)\n",
    "bootstrap_mod = pd.concat(all_bootstrap_mod, ignore_index=True)\n",
    "\n",
    "# По сегментам\n",
    "segment_results = {}\n",
    "for segment in ['high', 'low']:\n",
    "    emb_list, ds_list, mod_list = [], [], []\n",
    "    for metric in ['r2', 'rmse', 'medape']:\n",
    "        emb_list.append(bootstrap_embeddings(seg_preds[segment], metric, segment_name=segment))\n",
    "        ds_list.append(bootstrap_datasets(seg_preds[segment], metric, segment_name=segment))\n",
    "        mod_list.append(bootstrap_models(seg_preds[segment], metric, segment_name=segment))\n",
    "    segment_results[segment] = {\n",
    "        'bootstrap_emb': pd.concat(emb_list, ignore_index=True),\n",
    "        'bootstrap_ds': pd.concat(ds_list, ignore_index=True),\n",
    "        'bootstrap_mod': pd.concat(mod_list, ignore_index=True),\n",
    "        'metrics_overview': holdout_metrics_table(seg_preds[segment])\n",
    "    }\n",
    "\n",
    "# ============ [7] HTML-отчет ============\n",
    "style = \"\"\"\n",
    "<style>\n",
    "body { font-family: 'Times New Roman', Times, serif; font-size: 12pt; }\n",
    "table { margin-left: auto; margin-right: auto; border-collapse: collapse; }\n",
    "th, td { text-align: center; padding: 6px; }\n",
    "h1, h2, h3 { text-align: center; }\n",
    "</style>\n",
    "\"\"\"\n",
    "html_report = [style]\n",
    "html_report.append('<h1>Model Comparison Report</h1>')\n",
    "\n",
    "# ---- 1. Cross-validation ----\n",
    "html_report.append('<h2>1. Cross validation</h2>')\n",
    "html_report.append('<h3>1.1. Overview of cross-validation metrics (mean ±CI, original scale)</h3>')\n",
    "html_report.append(final_cv_orig.to_html(index=False, justify='center'))\n",
    "html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "html_report.append('<h3>1.2. Best embeddings per model and dataset (CV)</h3>')\n",
    "html_report.append(best_embeds_vis[['model','dataset','metric','best_embedding','best_score','second_embedding','second_score','stat_significant','p_value','comment']].to_html(index=False, justify='center'))\n",
    "html_report.append('<h3>1.3. Best datasets per model (CV)</h3>')\n",
    "html_report.append(best_datasets[['model','metric','best_dataset','best_score','second_dataset','second_score','stat_significant','p_value','comment']].to_html(index=False, justify='center'))\n",
    "html_report.append('<h3>1.4. Best models per metric (CV)</h3>')\n",
    "html_report.append(best_models[['metric','best_model','best_score','second_model','second_score','stat_significant','p_value','comment']].to_html(index=False, justify='center'))\n",
    "\n",
    "# ---- 2. Holdout bootstrap ----\n",
    "html_report.append('<h2>2. Holdout bootstrap evaluation</h2>')\n",
    "html_report.append('<h3>2.1. Overview of holdout metrics (bootstrap mean ±CI)</h3>')\n",
    "html_report.append(test_metrics_df_vis.to_html(index=False, justify='center'))\n",
    "html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "html_report.append('<h3>2.2. Bootstrap per embedding (test, per model+dataset)</h3>')\n",
    "html_report.append(bootstrap_emb.to_html(index=False, justify='center'))\n",
    "html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "html_report.append('<h3>2.3. Bootstrap per dataset (test, per model)</h3>')\n",
    "html_report.append(bootstrap_ds.to_html(index=False, justify='center'))\n",
    "html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "html_report.append('<h3>2.4. Bootstrap per model (test, all models)</h3>')\n",
    "html_report.append(bootstrap_mod.to_html(index=False, justify='center'))\n",
    "html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "\n",
    "# ---- 3. Segmented Holdout bootstrap ----\n",
    "html_report.append('<h2>3. Segmented Holdout bootstrap evaluation</h2>')\n",
    "for seg, seg_name in [('high', 'High-price segment (top 15%)'), ('low', 'Low-price segment (bottom 85%)')]:\n",
    "    html_report.append(f'<h3>3.1. Overview of holdout metrics ({seg_name}) (bootstrap mean ±CI)</h3>')\n",
    "    html_report.append(segment_results[seg]['metrics_overview'].to_html(index=False, justify='center'))\n",
    "    html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "    html_report.append(f'<h3>3.2. Test set bootstrap per embedding: {seg_name}</h3>')\n",
    "    html_report.append(segment_results[seg]['bootstrap_emb'].to_html(index=False, justify='center'))\n",
    "    html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "    html_report.append(f'<h3>3.3. Test set bootstrap per dataset: {seg_name}</h3>')\n",
    "    html_report.append(segment_results[seg]['bootstrap_ds'].to_html(index=False, justify='center'))\n",
    "    html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "    html_report.append(f'<h3>3.4. Test set bootstrap per model: {seg_name}</h3>')\n",
    "    html_report.append(segment_results[seg]['bootstrap_mod'].to_html(index=False, justify='center'))\n",
    "    html_report.append('<div><em>RMSE приведен в миллионах рублей.</em></div><br>')\n",
    "\n",
    "with open('model_comparison_report.html', 'w') as f:\n",
    "    f.write('\\n'.join(html_report))\n",
    "\n",
    "print('HTML report saved as model_comparison_report.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4i43Bfr8kEK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare, rankdata\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jinja2 import Template\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Функции для статистического анализа\n",
    "def friedman_test(data):\n",
    "    \"\"\"Выполняет тест Фридмана на данных (n_samples, n_methods)\"\"\"\n",
    "    try:\n",
    "        stat, p = friedmanchisquare(*data.T)\n",
    "        return stat, p\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def nemenyi_posthoc(ranks, n_methods, n_samples):\n",
    "    \"\"\"Выполняет пост-хок тест Немени на основе рангов\"\"\"\n",
    "    q_alpha = 2.569  # для alpha=0.05 и большого числа методов\n",
    "    cd = q_alpha * np.sqrt(n_methods * (n_methods + 1) / (6 * n_samples))\n",
    "    return cd\n",
    "\n",
    "def wilcoxon_test(data1, data2):\n",
    "    \"\"\"Парный тест Вилкоксона с поправкой на связанные ранги\"\"\"\n",
    "    try:\n",
    "        stat, p = stats.wilcoxon(data1, data2)\n",
    "        return stat, p\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def calculate_ranks(data):\n",
    "    \"\"\"Вычисляет ранги для данных (n_samples, n_methods)\"\"\"\n",
    "    return np.array([rankdata(row) for row in data])\n",
    "\n",
    "def load_cv_metrics(files):\n",
    "    \"\"\"Загружает метрики кросс-валидации из всех файлов\"\"\"\n",
    "    metrics = []\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Парсим информацию о модели и конфигурации из имени файла\n",
    "            filename = os.path.basename(file)\n",
    "            parts = filename.replace('metrics_', '').replace('_FINAL.csv', '').split('_')\n",
    "\n",
    "            model = parts[0]\n",
    "            dataset_type = None\n",
    "            embedding = None\n",
    "\n",
    "            # Определяем тип датасета и эмбеддинг\n",
    "            if 'categorical-only' in filename:\n",
    "                dataset_type = 'categorical-only'\n",
    "                embedding = 'none'\n",
    "            elif 'text-only' in filename:\n",
    "                dataset_type = 'text-only'\n",
    "                embedding = parts[-1] if parts[-1] in ['w2v', 'rubert', 'tfidf'] else 'unknown'\n",
    "            elif 'mixed' in filename:\n",
    "                dataset_type = 'mixed'\n",
    "                embedding = parts[-1] if parts[-1] in ['w2v', 'rubert', 'tfidf'] else 'unknown'\n",
    "            else:\n",
    "                print(f\"Не удалось определить тип датасета для файла: {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Читаем CSV файл\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            # Извлекаем метрики кросс-валидации\n",
    "            cv_metrics = df[df['type'] == 'cv_orig']\n",
    "\n",
    "            if cv_metrics.empty:\n",
    "                print(f\"Не найдены метрики cv_orig в файле: {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Собираем все метрики\n",
    "            for _, row in cv_metrics.iterrows():\n",
    "                metric_name = row['metric']\n",
    "                fold_values = [row[f'fold_{i}'] for i in range(5)]\n",
    "\n",
    "                metrics.append({\n",
    "                    'model': model,\n",
    "                    'dataset_type': dataset_type,\n",
    "                    'embedding': embedding,\n",
    "                    'metric': metric_name,\n",
    "                    'fold_0': fold_values[0],\n",
    "                    'fold_1': fold_values[1],\n",
    "                    'fold_2': fold_values[2],\n",
    "                    'fold_3': fold_values[3],\n",
    "                    'fold_4': fold_values[4],\n",
    "                    'mean': row['mean'],\n",
    "                    'std': row['std']\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при обработке файла {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def load_test_predictions(files):\n",
    "    \"\"\"Загружает предсказания на тестовой выборке\"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Парсим информацию о модели и конфигурации из пути к файлу\n",
    "            filename = os.path.basename(file)\n",
    "            parts = filename.replace('predictions_test_price_prediction_', '').replace('.csv', '').split('_')\n",
    "\n",
    "            model = parts[0]\n",
    "            dataset_type = None\n",
    "            embedding = None\n",
    "            pca = None\n",
    "            gate = None\n",
    "\n",
    "            # Определяем тип датасета и эмбеддинг\n",
    "            if 'categorical-only' in filename:\n",
    "                dataset_type = 'categorical-only'\n",
    "                embedding = 'none'\n",
    "                pca = parts[5] == 'True'\n",
    "                gate = parts[7] == 'True'\n",
    "            elif 'text-only' in filename:\n",
    "                dataset_type = 'text-only'\n",
    "                embedding = parts[2]\n",
    "                pca = parts[4] == 'True'\n",
    "                gate = parts[6] == 'True'\n",
    "            elif 'mixed' in filename:\n",
    "                dataset_type = 'mixed'\n",
    "                embedding = parts[2]\n",
    "                pca = parts[4] == 'True'\n",
    "                gate = parts[6] == 'True'\n",
    "            else:\n",
    "                print(f\"Не удалось определить тип датасета для файла: {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Читаем CSV файл\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            if 'true_price' not in df.columns or 'predicted_price' not in df.columns:\n",
    "                print(f\"Файл {filename} не содержит колонок true_price или predicted_price\")\n",
    "                continue\n",
    "\n",
    "            # Вычисляем метрики качества\n",
    "            true = df['true_price'].values\n",
    "            pred = df['predicted_price'].values\n",
    "\n",
    "            # R2 score\n",
    "            r2 = 1 - np.sum((true - pred)**2) / np.sum((true - np.mean(true))**2)\n",
    "\n",
    "            # RMSE\n",
    "            rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "\n",
    "            # SMAPE\n",
    "            smape = 100 * np.mean(2 * np.abs(pred - true) / (np.abs(pred) + np.abs(true)))\n",
    "\n",
    "            # MedAPE\n",
    "            medape = 100 * np.median(np.abs((true - pred) / true))\n",
    "\n",
    "            predictions.append({\n",
    "                'model': model,\n",
    "                'dataset_type': dataset_type,\n",
    "                'embedding': embedding,\n",
    "                'pca': pca,\n",
    "                'gate': gate,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                'smape': smape,\n",
    "                'medape': medape,\n",
    "                'file': filename\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при обработке файла {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "def compare_embeddings(cv_metrics_df):\n",
    "    \"\"\"Сравнивает техники эмбеддинга для каждой модели и типа датасета\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Группируем по модели и типу датасета\n",
    "    groups = cv_metrics_df.groupby(['model', 'dataset_type', 'metric'])\n",
    "\n",
    "    for (model, dataset_type, metric), group in groups:\n",
    "        # Пропускаем если нет нескольких техник эмбеддинга для сравнения\n",
    "        if len(group['embedding'].unique()) < 2:\n",
    "            print(f\"Недостаточно техник эмбеддинга для сравнения: model={model}, dataset={dataset_type}, metric={metric}\")\n",
    "            continue\n",
    "\n",
    "        # Подготовка данных для теста Фридмана (5 фолдов x N техник эмбеддинга)\n",
    "        embeddings = group['embedding'].unique()\n",
    "        n_embeddings = len(embeddings)\n",
    "        data = np.zeros((5, n_embeddings))  # 5 фолдов\n",
    "\n",
    "        for i, emb in enumerate(embeddings):\n",
    "            emb_data = group[group['embedding'] == emb]\n",
    "            if len(emb_data) != 1:\n",
    "                print(f\"Ожидалась 1 строка, получено {len(emb_data)} для model={model}, dataset={dataset_type}, metric={metric}, embedding={emb}\")\n",
    "                continue\n",
    "\n",
    "            data[:, i] = [\n",
    "                emb_data.iloc[0]['fold_0'],\n",
    "                emb_data.iloc[0]['fold_1'],\n",
    "                emb_data.iloc[0]['fold_2'],\n",
    "                emb_data.iloc[0]['fold_3'],\n",
    "                emb_data.iloc[0]['fold_4']\n",
    "            ]\n",
    "\n",
    "        # Вычисляем ранги\n",
    "        ranks = calculate_ranks(data)\n",
    "        avg_ranks = np.mean(ranks, axis=0)\n",
    "\n",
    "        # Тест Фридмана\n",
    "        friedman_stat, friedman_p = friedman_test(data)\n",
    "\n",
    "        # Критическая разница для теста Немени\n",
    "        cd = nemenyi_posthoc(avg_ranks, n_embeddings, 5) if not np.isnan(friedman_stat) else np.nan\n",
    "\n",
    "        # Сохраняем результаты\n",
    "        for i, emb in enumerate(embeddings):\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'dataset_type': dataset_type,\n",
    "                'metric': metric,\n",
    "                'embedding': emb,\n",
    "                'mean_score': np.mean(data[:, i]),\n",
    "                'std_score': np.std(data[:, i]),\n",
    "                'rank': avg_ranks[i],\n",
    "                'friedman_stat': friedman_stat,\n",
    "                'friedman_p': friedman_p,\n",
    "                'nemenyi_cd': cd\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def find_best_embeddings(embedding_comparison):\n",
    "    \"\"\"Определяет лучшие техники эмбеддинга на основе статистического сравнения\"\"\"\n",
    "    best_embeddings = []\n",
    "\n",
    "    groups = embedding_comparison.groupby(['model', 'dataset_type', 'metric'])\n",
    "\n",
    "    for (model, dataset_type, metric), group in groups:\n",
    "        # Если тест Фридмана не значим, выбираем с наилучшим средним\n",
    "        if group['friedman_p'].iloc[0] >= 0.05:\n",
    "            best = group.loc[group['mean_score'].idxmax() if metric == 'r2' else group['mean_score'].idxmin()]\n",
    "            best_embeddings.append({\n",
    "                'model': model,\n",
    "                'dataset_type': dataset_type,\n",
    "                'metric': metric,\n",
    "                'best_embedding': best['embedding'],\n",
    "                'reason': 'best_mean (no significant difference)',\n",
    "                'mean_score': best['mean_score'],\n",
    "                'friedman_p': best['friedman_p']\n",
    "            })\n",
    "        else:\n",
    "            # Если тест значим, проверяем какие техники не отличаются значительно от лучшей\n",
    "            min_rank = group['rank'].min()\n",
    "            cd = group['nemenyi_cd'].iloc[0]\n",
    "\n",
    "            # Все техники с рангом в пределах CD от минимального\n",
    "            best_candidates = group[group['rank'] - min_rank <= cd]\n",
    "\n",
    "            # Из них выбираем с наилучшим средним\n",
    "            best = best_candidates.loc[best_candidates['mean_score'].idxmax() if metric == 'r2' else best_candidates['mean_score'].idxmin()]\n",
    "\n",
    "            best_embeddings.append({\n",
    "                'model': model,\n",
    "                'dataset_type': dataset_type,\n",
    "                'metric': metric,\n",
    "                'best_embedding': best['embedding'],\n",
    "                'reason': f'statistically best (p={best[\"friedman_p\"]:.3f}, CD={cd:.2f})',\n",
    "                'mean_score': best['mean_score'],\n",
    "                'friedman_p': best['friedman_p']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(best_embeddings)\n",
    "\n",
    "def compare_datasets(cv_metrics_df, best_embeddings):\n",
    "    \"\"\"Сравнивает типы датасетов для каждой модели с использованием лучших техник эмбеддинга\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Фильтруем метрики, оставляя только лучшие техники эмбеддинга\n",
    "    filtered_metrics = []\n",
    "\n",
    "    for _, row in best_embeddings.iterrows():\n",
    "        mask = (\n",
    "            (cv_metrics_df['model'] == row['model']) &\n",
    "            (cv_metrics_df['dataset_type'] == row['dataset_type']) &\n",
    "            (cv_metrics_df['metric'] == row['metric']) &\n",
    "            (cv_metrics_df['embedding'] == row['best_embedding'])\n",
    "        )\n",
    "\n",
    "        filtered = cv_metrics_df[mask]\n",
    "        if not filtered.empty:\n",
    "            filtered_metrics.append(filtered.iloc[0])\n",
    "        else:\n",
    "            print(f\"Не найдены метрики для model={row['model']}, dataset={row['dataset_type']}, metric={row['metric']}, embedding={row['best_embedding']}\")\n",
    "\n",
    "    filtered_metrics_df = pd.DataFrame(filtered_metrics)\n",
    "\n",
    "    # Группируем по модели и метрике\n",
    "    groups = filtered_metrics_df.groupby(['model', 'metric'])\n",
    "\n",
    "    for (model, metric), group in groups:\n",
    "        # Пропускаем если нет нескольких типов датасетов для сравнения\n",
    "        if len(group['dataset_type'].unique()) < 2:\n",
    "            print(f\"Недостаточно типов датасетов для сравнения: model={model}, metric={metric}\")\n",
    "            continue\n",
    "\n",
    "        # Подготовка данных для теста Фридмана (5 фолдов x N типов датасетов)\n",
    "        datasets = group['dataset_type'].unique()\n",
    "        n_datasets = len(datasets)\n",
    "        data = np.zeros((5, n_datasets))  # 5 фолдов\n",
    "\n",
    "        for i, ds in enumerate(datasets):\n",
    "            ds_data = group[group['dataset_type'] == ds]\n",
    "            if len(ds_data) != 1:\n",
    "                print(f\"Ожидалась 1 строка, получено {len(ds_data)} для model={model}, metric={metric}, dataset={ds}\")\n",
    "                continue\n",
    "\n",
    "            data[:, i] = [\n",
    "                ds_data.iloc[0]['fold_0'],\n",
    "                ds_data.iloc[0]['fold_1'],\n",
    "                ds_data.iloc[0]['fold_2'],\n",
    "                ds_data.iloc[0]['fold_3'],\n",
    "                ds_data.iloc[0]['fold_4']\n",
    "            ]\n",
    "\n",
    "        # Вычисляем ранги\n",
    "        ranks = calculate_ranks(data)\n",
    "        avg_ranks = np.mean(ranks, axis=0)\n",
    "\n",
    "        # Тест Фридмана\n",
    "        friedman_stat, friedman_p = friedman_test(data)\n",
    "\n",
    "        # Критическая разница для теста Немени\n",
    "        cd = nemenyi_posthoc(avg_ranks, n_datasets, 5) if not np.isnan(friedman_stat) else np.nan\n",
    "\n",
    "        # Сохраняем результаты\n",
    "        for i, ds in enumerate(datasets):\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'dataset_type': ds,\n",
    "                'mean_score': np.mean(data[:, i]),\n",
    "                'std_score': np.std(data[:, i]),\n",
    "                'rank': avg_ranks[i],\n",
    "                'friedman_stat': friedman_stat,\n",
    "                'friedman_p': friedman_p,\n",
    "                'nemenyi_cd': cd\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def find_best_datasets(dataset_comparison):\n",
    "    \"\"\"Определяет лучшие типы датасетов на основе статистического сравнения\"\"\"\n",
    "    best_datasets = []\n",
    "\n",
    "    groups = dataset_comparison.groupby(['model', 'metric'])\n",
    "\n",
    "    for (model, metric), group in groups:\n",
    "        # Если тест Фридмана не значим, выбираем с наилучшим средним\n",
    "        if group['friedman_p'].iloc[0] >= 0.05:\n",
    "            best = group.loc[group['mean_score'].idxmax() if metric == 'r2' else group['mean_score'].idxmin()]\n",
    "            best_datasets.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'best_dataset': best['dataset_type'],\n",
    "                'reason': 'best_mean (no significant difference)',\n",
    "                'mean_score': best['mean_score'],\n",
    "                'friedman_p': best['friedman_p']\n",
    "            })\n",
    "        else:\n",
    "            # Если тест значим, проверяем какие типы не отличаются значительно от лучшего\n",
    "            min_rank = group['rank'].min()\n",
    "            cd = group['nemenyi_cd'].iloc[0]\n",
    "\n",
    "            # Все типы с рангом в пределах CD от минимального\n",
    "            best_candidates = group[group['rank'] - min_rank <= cd]\n",
    "\n",
    "            # Из них выбираем с наилучшим средним\n",
    "            best = best_candidates.loc[best_candidates['mean_score'].idxmax() if metric == 'r2' else best_candidates['mean_score'].idxmin()]\n",
    "\n",
    "            best_datasets.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'best_dataset': best['dataset_type'],\n",
    "                'reason': f'statistically best (p={best[\"friedman_p\"]:.3f}, CD={cd:.2f})',\n",
    "                'mean_score': best['mean_score'],\n",
    "                'friedman_p': best['friedman_p']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(best_datasets)\n",
    "\n",
    "def compare_models(cv_metrics_df, best_embeddings, best_datasets):\n",
    "    \"\"\"Сравнивает модели между собой с использованием их лучших конфигураций\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Фильтруем метрики, оставляя только лучшие конфигурации (эмбеддинг + датасет)\n",
    "    filtered_metrics = []\n",
    "\n",
    "    for _, row in best_datasets.iterrows():\n",
    "        # Находим лучший эмбеддинг для этой модели и датасета\n",
    "        best_emb = best_embeddings[\n",
    "            (best_embeddings['model'] == row['model']) &\n",
    "            (best_embeddings['dataset_type'] == row['best_dataset']) &\n",
    "            (best_embeddings['metric'] == row['metric'])\n",
    "        ]\n",
    "\n",
    "        if best_emb.empty:\n",
    "            print(f\"Не найден лучший эмбеддинг для model={row['model']}, dataset={row['best_dataset']}, metric={row['metric']}\")\n",
    "            continue\n",
    "\n",
    "        best_emb = best_emb.iloc[0]\n",
    "\n",
    "        # Находим соответствующие метрики\n",
    "        mask = (\n",
    "            (cv_metrics_df['model'] == row['model']) &\n",
    "            (cv_metrics_df['dataset_type'] == row['best_dataset']) &\n",
    "            (cv_metrics_df['metric'] == row['metric']) &\n",
    "            (cv_metrics_df['embedding'] == best_emb['best_embedding'])\n",
    "        )\n",
    "\n",
    "        filtered = cv_metrics_df[mask]\n",
    "        if not filtered.empty:\n",
    "            filtered_metrics.append(filtered.iloc[0])\n",
    "        else:\n",
    "            print(f\"Не найдены метрики для model={row['model']}, dataset={row['best_dataset']}, metric={row['metric']}, embedding={best_emb['best_embedding']}\")\n",
    "\n",
    "    filtered_metrics_df = pd.DataFrame(filtered_metrics)\n",
    "\n",
    "    # Группируем по метрике\n",
    "    groups = filtered_metrics_df.groupby('metric')\n",
    "\n",
    "    for metric, group in groups:\n",
    "        # Пропускаем если нет нескольких моделей для сравнения\n",
    "        if len(group['model'].unique()) < 2:\n",
    "            print(f\"Недостаточно моделей для сравнения по метрике: {metric}\")\n",
    "            continue\n",
    "\n",
    "        # Подготовка данных для теста Фридмана (5 фолдов x N моделей)\n",
    "        models = group['model'].unique()\n",
    "        n_models = len(models)\n",
    "        data = np.zeros((5, n_models))  # 5 фолдов\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            model_data = group[group['model'] == model]\n",
    "            if len(model_data) != 1:\n",
    "                print(f\"Ожидалась 1 строка, получено {len(model_data)} для metric={metric}, model={model}\")\n",
    "                continue\n",
    "\n",
    "            data[:, i] = [\n",
    "                model_data.iloc[0]['fold_0'],\n",
    "                model_data.iloc[0]['fold_1'],\n",
    "                model_data.iloc[0]['fold_2'],\n",
    "                model_data.iloc[0]['fold_3'],\n",
    "                model_data.iloc[0]['fold_4']\n",
    "            ]\n",
    "\n",
    "        # Вычисляем ранги\n",
    "        ranks = calculate_ranks(data)\n",
    "        avg_ranks = np.mean(ranks, axis=0)\n",
    "\n",
    "        # Тест Фридмана\n",
    "        friedman_stat, friedman_p = friedman_test(data)\n",
    "\n",
    "        # Критическая разница для теста Немени\n",
    "        cd = nemenyi_posthoc(avg_ranks, n_models, 5) if not np.isnan(friedman_stat) else np.nan\n",
    "\n",
    "        # Сохраняем результаты\n",
    "        for i, model in enumerate(models):\n",
    "            results.append({\n",
    "                'metric': metric,\n",
    "                'model': model,\n",
    "                'mean_score': np.mean(data[:, i]),\n",
    "                'std_score': np.std(data[:, i]),\n",
    "                'rank': avg_ranks[i],\n",
    "                'friedman_stat': friedman_stat,\n",
    "                'friedman_p': friedman_p,\n",
    "                'nemenyi_cd': cd\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def find_best_models(model_comparison):\n",
    "    \"\"\"Определяет лучшие модели на основе статистического сравнения\"\"\"\n",
    "    best_models = []\n",
    "\n",
    "    groups = model_comparison.groupby('metric')\n",
    "\n",
    "    for metric, group in groups:\n",
    "        # Если тест Фридмана не значим, выбираем с наилучшим средним\n",
    "        if group['friedman_p'].iloc[0] >= 0.05:\n",
    "            best = group.loc[group['mean_score'].idxmax() if metric == 'r2' else group['mean_score'].idxmin()]\n",
    "            best_models.append({\n",
    "                'metric': metric,\n",
    "                'best_model': best['model'],\n",
    "                'reason': 'best_mean (no significant difference)',\n",
    "                'mean_score': best['mean_score'],\n",
    "                'friedman_p': best['friedman_p']\n",
    "            })\n",
    "        else:\n",
    "            # Если тест значим, проверяем какие модели не отличаются значительно от лучшей\n",
    "            min_rank = group['rank'].min()\n",
    "            cd = group['nemenyi_cd'].iloc[0]\n",
    "\n",
    "            # Все модели с рангом в пределах CD от минимального\n",
    "            best_candidates = group[group['rank'] - min_rank <= cd]\n",
    "\n",
    "            # Из них выбираем с наилучшим средним\n",
    "            best = best_candidates.loc[best_candidates['mean_score'].idxmax() if metric == 'r2' else best_candidates['mean_score'].idxmin()]\n",
    "\n",
    "            best_models.append({\n",
    "                'metric': metric,\n",
    "                'best_model': best['model'],\n",
    "                'reason': f'statistically best (p={best[\"friedman_p\"]:.3f}, CD={cd:.2f})',\n",
    "                'mean_score': best['mean_score'],\n",
    "                'friedman_p': best['friedman_p']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(best_models)\n",
    "\n",
    "def compare_test_predictions(test_predictions_df, best_embeddings, best_datasets):\n",
    "    \"\"\"Сравнивает предсказания на тестовой выборке для лучших конфигураций\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Фильтруем предсказания, оставляя только лучшие конфигурации\n",
    "    filtered_predictions = []\n",
    "\n",
    "    for _, row in best_datasets.iterrows():\n",
    "        # Находим лучший эмбеддинг для этой модели и датасета\n",
    "        best_emb = best_embeddings[\n",
    "            (best_embeddings['model'] == row['model']) &\n",
    "            (best_embeddings['dataset_type'] == row['best_dataset']) &\n",
    "            (best_embeddings['metric'] == row['metric'])\n",
    "        ]\n",
    "\n",
    "        if best_emb.empty:\n",
    "            print(f\"Не найден лучший эмбеддинг для model={row['model']}, dataset={row['best_dataset']}, metric={row['metric']}\")\n",
    "            continue\n",
    "\n",
    "        best_emb = best_emb.iloc[0]\n",
    "\n",
    "        # Находим соответствующие предсказания на тесте\n",
    "        mask = (\n",
    "            (test_predictions_df['model'] == row['model']) &\n",
    "            (test_predictions_df['dataset_type'] == row['best_dataset']) &\n",
    "            (test_predictions_df['embedding'] == best_emb['best_embedding'])\n",
    "        )\n",
    "\n",
    "        filtered = test_predictions_df[mask]\n",
    "        if not filtered.empty:\n",
    "            # Берем первую попавшуюся конфигурацию (PCA и gate могут различаться)\n",
    "            filtered_predictions.append(filtered.iloc[0])\n",
    "        else:\n",
    "            print(f\"Не найдены предсказания для model={row['model']}, dataset={row['best_dataset']}, embedding={best_emb['best_embedding']}\")\n",
    "\n",
    "    filtered_predictions_df = pd.DataFrame(filtered_predictions)\n",
    "\n",
    "    # Сравниваем модели по каждой метрике с помощью теста Дайболда-Мариано\n",
    "    metrics = ['r2', 'rmse', 'smape', 'medape']\n",
    "    model_pairs = []\n",
    "\n",
    "    # Генерируем все уникальные пары моделей\n",
    "    models = filtered_predictions_df['model'].unique()\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model_pairs.append((models[i], models[j]))\n",
    "\n",
    "    for metric in metrics:\n",
    "        for model1, model2 in model_pairs:\n",
    "            # Получаем предсказания для обеих моделей\n",
    "            pred1 = filtered_predictions_df[filtered_predictions_df['model'] == model1]\n",
    "            pred2 = filtered_predictions_df[filtered_predictions_df['model'] == model2]\n",
    "\n",
    "            if len(pred1) == 0 or len(pred2) == 0:\n",
    "                print(f\"Нет данных для сравнения {model1} vs {model2} по метрике {metric}\")\n",
    "                continue\n",
    "\n",
    "            # Загружаем файлы с предсказаниями\n",
    "            try:\n",
    "                df1 = pd.read_csv(pred1.iloc[0]['file'])\n",
    "                df2 = pd.read_csv(pred2.iloc[0]['file'])\n",
    "\n",
    "                true = df1['true_price'].values\n",
    "                pred1_vals = df1['predicted_price'].values\n",
    "                pred2_vals = df2['predicted_price'].values\n",
    "\n",
    "                # Вычисляем потери для теста Дайболда-Мариано\n",
    "                if metric == 'r2':\n",
    "                    # Для R2 используем квадраты ошибок\n",
    "                    loss1 = (true - pred1_vals)**2\n",
    "                    loss2 = (true - pred2_vals)**2\n",
    "                elif metric == 'rmse':\n",
    "                    loss1 = np.abs(true - pred1_vals)\n",
    "                    loss2 = np.abs(true - pred2_vals)\n",
    "                elif metric in ['smape', 'medape']:\n",
    "                    loss1 = np.abs((true - pred1_vals) / true)\n",
    "                    loss2 = np.abs((true - pred2_vals) / true)\n",
    "\n",
    "                # Тест Дайболда-Мариано\n",
    "                dm_stat, dm_p = stats.ttest_rel(loss1, loss2)\n",
    "\n",
    "                results.append({\n",
    "                    'metric': metric,\n",
    "                    'model1': model1,\n",
    "                    'model2': model2,\n",
    "                    'model1_score': pred1.iloc[0][metric],\n",
    "                    'model2_score': pred2.iloc[0][metric],\n",
    "                    'dm_stat': dm_stat,\n",
    "                    'dm_p': dm_p,\n",
    "                    'significant': dm_p < 0.05\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при сравнении {model1} vs {model2} по метрике {metric}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def generate_html_report(best_embeddings, best_datasets, best_models,\n",
    "                        embedding_comparison, dataset_comparison, model_comparison,\n",
    "                        test_comparison, cv_metrics_df, test_predictions_df):\n",
    "    \"\"\"Генерирует HTML отчет с результатами анализа\"\"\"\n",
    "\n",
    "    # Создаем графики\n",
    "    plots = {}\n",
    "\n",
    "    # 1. Графики сравнения эмбеддингов\n",
    "    for (model, dataset_type), group in embedding_comparison.groupby(['model', 'dataset_type']):\n",
    "        for metric in group['metric'].unique():\n",
    "            subset = group[group['metric'] == metric]\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='embedding', y='mean_score', data=subset)\n",
    "            plt.title(f'{model} - {dataset_type} - {metric}\\nFriedman p={subset[\"friedman_p\"].iloc[0]:.3f}')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_name = f'embedding_{model}_{dataset_type}_{metric}'.replace('-', '_')\n",
    "            plots[plot_name] = plt.gcf()\n",
    "            plt.close()\n",
    "\n",
    "    # 2. Графики сравнения датасетов\n",
    "    for model, group in dataset_comparison.groupby('model'):\n",
    "        for metric in group['metric'].unique():\n",
    "            subset = group[group['metric'] == metric]\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='dataset_type', y='mean_score', data=subset)\n",
    "            plt.title(f'{model} - {metric}\\nFriedman p={subset[\"friedman_p\"].iloc[0]:.3f}')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_name = f'dataset_{model}_{metric}'.replace('-', '_')\n",
    "            plots[plot_name] = plt.gcf()\n",
    "            plt.close()\n",
    "\n",
    "    # 3. Графики сравнения моделей\n",
    "    for metric in model_comparison['metric'].unique():\n",
    "        subset = model_comparison[model_comparison['metric'] == metric]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='model', y='mean_score', data=subset)\n",
    "        plt.title(f'Model comparison - {metric}\\nFriedman p={subset[\"friedman_p\"].iloc[0]:.3f}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_name = f'model_{metric}'\n",
    "        plots[plot_name] = plt.gcf()\n",
    "        plt.close()\n",
    "\n",
    "    # Сохраняем графики в файлы\n",
    "    plot_files = {}\n",
    "    for name, fig in plots.items():\n",
    "        filename = f'{name}.png'\n",
    "        fig.savefig(filename)\n",
    "        plot_files[name] = filename\n",
    "\n",
    "    # Подготавливаем данные для отчета\n",
    "    report_data = {\n",
    "        'best_embeddings': best_embeddings.to_dict('records'),\n",
    "        'best_datasets': best_datasets.to_dict('records'),\n",
    "        'best_models': best_models.to_dict('records'),\n",
    "        'test_comparison': test_comparison.to_dict('records'),\n",
    "        'plot_files': plot_files\n",
    "    }\n",
    "\n",
    "    # HTML шаблон\n",
    "    html_template = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Model Comparison Report</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "            h1 { color: #2c3e50; }\n",
    "            h2 { color: #3498db; margin-top: 30px; }\n",
    "            h3 { color: #16a085; }\n",
    "            table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }\n",
    "            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "            th { background-color: #f2f2f2; }\n",
    "            tr:nth-child(even) { background-color: #f9f9f9; }\n",
    "            .plot { margin: 20px 0; text-align: center; }\n",
    "            .plot img { max-width: 80%; border: 1px solid #ddd; }\n",
    "            .significant { color: #e74c3c; font-weight: bold; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Statistical Model Comparison Report</h1>\n",
    "\n",
    "        <h2>1. Best Embedding Techniques</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Model</th>\n",
    "                <th>Dataset Type</th>\n",
    "                <th>Metric</th>\n",
    "                <th>Best Embedding</th>\n",
    "                <th>Mean Score</th>\n",
    "                <th>Friedman p-value</th>\n",
    "                <th>Selection Reason</th>\n",
    "            </tr>\n",
    "            {% for item in best_embeddings %}\n",
    "            <tr>\n",
    "                <td>{{ item.model }}</td>\n",
    "                <td>{{ item.dataset_type }}</td>\n",
    "                <td>{{ item.metric }}</td>\n",
    "                <td>{{ item.best_embedding }}</td>\n",
    "                <td>{{ \"%.4f\"|format(item.mean_score) }}</td>\n",
    "                <td>{{ \"%.3f\"|format(item.friedman_p) }}</td>\n",
    "                <td>{{ item.reason }}</td>\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "\n",
    "        {% for plot in plot_files if plot.startswith('embedding_') %}\n",
    "        <div class=\"plot\">\n",
    "            <img src=\"{{ plot_files[plot] }}\" alt=\"{{ plot }}\">\n",
    "        </div>\n",
    "        {% endfor %}\n",
    "\n",
    "        <h2>2. Best Dataset Types</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Model</th>\n",
    "                <th>Metric</th>\n",
    "                <th>Best Dataset</th>\n",
    "                <th>Mean Score</th>\n",
    "                <th>Friedman p-value</th>\n",
    "                <th>Selection Reason</th>\n",
    "            </tr>\n",
    "            {% for item in best_datasets %}\n",
    "            <tr>\n",
    "                <td>{{ item.model }}</td>\n",
    "                <td>{{ item.metric }}</td>\n",
    "                <td>{{ item.best_dataset }}</td>\n",
    "                <td>{{ \"%.4f\"|format(item.mean_score) }}</td>\n",
    "                <td>{{ \"%.3f\"|format(item.friedman_p) }}</td>\n",
    "                <td>{{ item.reason }}</td>\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "\n",
    "        {% for plot in plot_files if plot.startswith('dataset_') %}\n",
    "        <div class=\"plot\">\n",
    "            <img src=\"{{ plot_files[plot] }}\" alt=\"{{ plot }}\">\n",
    "        </div>\n",
    "        {% endfor %}\n",
    "\n",
    "        <h2>3. Best Models</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Metric</th>\n",
    "                <th>Best Model</th>\n",
    "                <th>Mean Score</th>\n",
    "                <th>Friedman p-value</th>\n",
    "                <th>Selection Reason</th>\n",
    "            </tr>\n",
    "            {% for item in best_models %}\n",
    "            <tr>\n",
    "                <td>{{ item.metric }}</td>\n",
    "                <td>{{ item.best_model }}</td>\n",
    "                <td>{{ \"%.4f\"|format(item.mean_score) }}</td>\n",
    "                <td>{{ \"%.3f\"|format(item.friedman_p) }}</td>\n",
    "                <td>{{ item.reason }}</td>\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "\n",
    "        {% for plot in plot_files if plot.startswith('model_') %}\n",
    "        <div class=\"plot\">\n",
    "            <img src=\"{{ plot_files[plot] }}\" alt=\"{{ plot }}\">\n",
    "        </div>\n",
    "        {% endfor %}\n",
    "\n",
    "        <h2>4. Test Set Comparisons</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Metric</th>\n",
    "                <th>Model 1</th>\n",
    "                <th>Model 2</th>\n",
    "                <th>Model 1 Score</th>\n",
    "                <th>Model 2 Score</th>\n",
    "                <th>DM p-value</th>\n",
    "                <th>Significant</th>\n",
    "            </tr>\n",
    "            {% for item in test_comparison %}\n",
    "            <tr>\n",
    "                <td>{{ item.metric }}</td>\n",
    "                <td>{{ item.model1 }}</td>\n",
    "                <td>{{ item.model2 }}</td>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vxVwmGtwkp_"
   },
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_ot61RBwlrv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import Optional\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RuBertTiny2Embedder(BaseEstimator, TransformerMixin):\n",
    "    DEFAULT_MODEL_NAME = \"cointegrated/rubert-tiny2\"  # Фиксированная модель по умолчанию\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_length: int = 512,\n",
    "        batch_size: int = 128,\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 5,\n",
    "        device: Optional[str] = None,\n",
    "        use_cv: bool = False,\n",
    "        pooling_type: str = \"cls\",  # \"cls\", \"mean\", \"max\", \"weighted\"\n",
    "        model_name: Optional[str] = None  # Опциональное переопределение модели\n",
    "    ):\n",
    "        assert pooling_type in [\"cls\", \"mean\", \"max\", \"weighted\"], \\\n",
    "            f\"pooling_type must be one of: cls, mean, max, weighted. Got {pooling_type}\"\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.use_cv = use_cv\n",
    "        self.pooling_type = pooling_type\n",
    "        self.model_name = model_name or self.DEFAULT_MODEL_NAME  # Гарантированно правильное имя модели\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    class _PricePredictionModel(nn.Module):\n",
    "        def __init__(self, model_name: str, pooling_type: str):\n",
    "            super().__init__()\n",
    "            self.bert = AutoModel.from_pretrained(model_name)\n",
    "            self.pooling_type = pooling_type\n",
    "            self.hidden_size = self.bert.config.hidden_size\n",
    "            self.regressor = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "            if self.pooling_type == \"weighted\":\n",
    "                self.attention = nn.Sequential(\n",
    "                    nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(self.hidden_size, 1)\n",
    "                )\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False\n",
    "            )\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "\n",
    "            if self.pooling_type == \"cls\":\n",
    "                pooled = last_hidden[:, 0, :]\n",
    "\n",
    "            elif self.pooling_type == \"mean\":\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden * input_mask_expanded, dim=1)\n",
    "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                pooled = sum_embeddings / sum_mask\n",
    "\n",
    "            elif self.pooling_type == \"max\":\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "                last_hidden[input_mask_expanded == 0] = -torch.inf\n",
    "                pooled = torch.max(last_hidden, dim=1)[0]\n",
    "\n",
    "            elif self.pooling_type == \"weighted\":\n",
    "                weights = self.attention(last_hidden).squeeze(-1)\n",
    "                weights = weights.masked_fill(attention_mask == 0, -torch.inf)\n",
    "                weights = torch.softmax(weights, dim=1)\n",
    "                pooled = torch.sum(last_hidden * weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "            return self.regressor(pooled), pooled\n",
    "\n",
    "    class _TextPriceDataset(Dataset):\n",
    "        def __init__(self, texts, prices, tokenizer, max_length):\n",
    "            self.texts = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "            self.prices = prices.tolist() if hasattr(prices, 'tolist') else list(prices)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            encoding = self.tokenizer(\n",
    "                str(self.texts[idx]),\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'price': torch.tensor(float(self.prices[idx]), dtype=torch.float32)\n",
    "            }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit model on training data\"\"\"\n",
    "        # Преобразование входных данных\n",
    "        X = self._convert_to_list(X)\n",
    "        y = self._convert_to_list(y) if y is not None else None\n",
    "\n",
    "        # Инициализация токенизатора и модели\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = self._PricePredictionModel(\n",
    "            model_name=self.model_name,\n",
    "            pooling_type=self.pooling_type\n",
    "        ).to(self.device)\n",
    "\n",
    "        if y is not None:\n",
    "            self._train_model(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_token_importance(self, texts, top_n=20):\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"Model not trained yet. Call fit() first.\")\n",
    "        if self.pooling_type != \"weighted\":\n",
    "            raise ValueError(\"Token importance доступна только для weighted pooling\")\n",
    "\n",
    "        self.model.eval()\n",
    "        all_importances = []\n",
    "\n",
    "        for text in texts:\n",
    "            # tokenize\n",
    "            enc = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "\n",
    "            # get last hidden states\n",
    "            with torch.no_grad():\n",
    "                last_hidden = self.model.bert(\n",
    "                    input_ids=enc['input_ids'],\n",
    "                    attention_mask=enc['attention_mask']\n",
    "                ).last_hidden_state  # (1, seq_len, hidden_size)\n",
    "\n",
    "                # compute raw attention scores\n",
    "                scores = self.model.attention(last_hidden)         # (1, seq_len, 1)\n",
    "                scores = scores.squeeze(-1)                        # (1, seq_len)\n",
    "                scores = scores.masked_fill(enc['attention_mask']==0, -1e9)\n",
    "                weights = torch.softmax(scores, dim=1).cpu().numpy()[0]  # (seq_len,)\n",
    "\n",
    "            # map back to tokens\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(enc['input_ids'][0])\n",
    "            token_scores = list(zip(tokens, weights))\n",
    "\n",
    "            # take the top_n and strip “##” prefixes\n",
    "            top = sorted(token_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "            top = [(t.replace('##', ''), float(w)) for t, w in top if len(t.replace('##', '')) >= 4]\n",
    "            all_importances.append(top)\n",
    "\n",
    "        return all_importances\n",
    "\n",
    "\n",
    "    def _convert_to_list(self, data):\n",
    "        \"\"\"Convert input data to list\"\"\"\n",
    "        if data is None:\n",
    "            return None\n",
    "        if hasattr(data, 'iloc'):  # pandas DataFrame/Series\n",
    "            return data.iloc[:, 0].tolist() if data.ndim > 1 else data.tolist()\n",
    "        if hasattr(data, 'tolist'):\n",
    "            return data.tolist()\n",
    "        return list(data)\n",
    "\n",
    "    def _train_model(self, X, y):\n",
    "        \"\"\"Internal training procedure\"\"\"\n",
    "        dataset = self._TextPriceDataset(X, y, self.tokenizer, self.max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                inputs = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                predictions, _ = self.model(inputs['input_ids'], inputs['attention_mask'])\n",
    "                loss = loss_fn(predictions.squeeze(), inputs['price'])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs} - Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Generate embeddings for input texts\"\"\"\n",
    "        X = self._convert_to_list(X)\n",
    "\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "\n",
    "        dataset = self._TextPriceDataset(X, [0]*len(X), self.tokenizer, self.max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                inputs = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                _, emb = self.model(inputs['input_ids'], inputs['attention_mask'])\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model to directory\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Сохраняем компоненты модели\n",
    "        self.model.bert.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "\n",
    "        # Сохраняем конфигурацию\n",
    "        config = {\n",
    "            'max_length': self.max_length,\n",
    "            'batch_size': self.batch_size,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'num_epochs': self.num_epochs,\n",
    "            'device': str(self.device),\n",
    "            'use_cv': self.use_cv,\n",
    "            'pooling_type': self.pooling_type,\n",
    "            'model_name': self.model_name\n",
    "        }\n",
    "        with open(os.path.join(path, 'config.json'), 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "\n",
    "        # Сохраняем веса головки\n",
    "        state_dict = {\n",
    "            'regressor_state_dict': self.model.regressor.state_dict()\n",
    "        }\n",
    "        if self.pooling_type == \"weighted\":\n",
    "            state_dict['attention_state_dict'] = self.model.attention.state_dict()\n",
    "\n",
    "        torch.save(state_dict, os.path.join(path, 'head_weights.pt'))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Load model from directory\"\"\"\n",
    "        with open(os.path.join(path, 'config.json')) as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Создаем экземпляр с сохраненными параметрами\n",
    "        instance = cls(\n",
    "            max_length=config['max_length'],\n",
    "            batch_size=config['batch_size'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            num_epochs=config['num_epochs'],\n",
    "            device=config['device'],\n",
    "            use_cv=config.get('use_cv', False),\n",
    "            pooling_type=config['pooling_type'],\n",
    "            model_name=config['model_name']  # Важно: используем сохраненное имя модели\n",
    "        )\n",
    "\n",
    "        # Загружаем токенизатор и модель\n",
    "        instance.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        instance.model = instance._PricePredictionModel(\n",
    "            model_name=path,  # Загружаем из локальной папки\n",
    "            pooling_type=config['pooling_type']\n",
    "        ).to(instance.device)\n",
    "\n",
    "        # Загружаем веса головки\n",
    "        head_weights = torch.load(\n",
    "            os.path.join(path, 'head_weights.pt'),\n",
    "            map_location=instance.device\n",
    "        )\n",
    "        instance.model.regressor.load_state_dict(head_weights['regressor_state_dict'])\n",
    "        if config['pooling_type'] == \"weighted\":\n",
    "            instance.model.attention.load_state_dict(head_weights['attention_state_dict'])\n",
    "\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5R50T-WNwrB1",
    "outputId": "884389f7-6a85-4c9e-87c8-59e875119e1e"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/price_prediction_data/raw_data_parsed_desc_preprocessed_full.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "process = DataProcessingPipeline(\n",
    "            df,\n",
    "            log_needed=True,\n",
    "            norm_needed=True,\n",
    "            one_hot_only=True,\n",
    "            train=True,\n",
    "            use_hex_features=True,\n",
    "            hex_resolution=10\n",
    "        )\n",
    "df = process.preprocess_base()\n",
    "\n",
    "\n",
    "df['price_category'] = pd.qcut(df['price'], q=10, labels=False)\n",
    "\n",
    "# Теперь можно стратифицировать по price_category\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['price_category']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_process = DataProcessingPipeline(\n",
    "            df,\n",
    "            log_needed=True,\n",
    "            norm_needed=True,\n",
    "            one_hot_only=True,\n",
    "            train=True,\n",
    "            use_hex_features=True,\n",
    "            hex_resolution=10\n",
    "      )\n",
    "train_result = train_process.prepare_for_model()\n",
    "train_processed = train_result['processed_df']\n",
    "\n",
    "print(list(train_processed.columns))\n",
    "\n",
    "# Обработка тестовых данных\n",
    "test_process = DataProcessingPipeline(\n",
    "    test_df,\n",
    "    log_needed=True,\n",
    "    norm_needed=True,\n",
    "    one_hot_only=True,\n",
    "    train=False,\n",
    "    outlier_bounds=train_result['outlier_bounds'],\n",
    "    scaler=train_result['scaler'],\n",
    "    lat_long_scaler=train_result['lat_long_scaler'],\n",
    "    use_hex_features=True,\n",
    "    hex_resolution=10,\n",
    "    hex_stats=train_process.hex_stats,\n",
    "    global_median_ppsm=train_process.global_median_ppsm,\n",
    "    global_median_ppland=train_process.global_median_ppland,\n",
    "    global_median_price=train_process.global_median_price\n",
    ")\n",
    "test_processed = test_process.prepare_for_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "8toiD6IcCs2w",
    "outputId": "fc9e9a2b-33f2-447d-ec11-fa83ce22d4de"
   },
   "outputs": [],
   "source": [
    "X_train = train_processed['description_raw']\n",
    "y_train = train_processed['price']\n",
    "\n",
    "\n",
    "bert_embedder = RuBertTiny2Embedder(\n",
    "    max_length=512,\n",
    "    batch_size=8,\n",
    "    pooling_type='weighted',  # ключевое изменение!\n",
    "    learning_rate=2.196386920803346e-05,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "# 2. Обучение на тренировочных данных\n",
    "bert_embedder.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoyAqm6pGsmU"
   },
   "outputs": [],
   "source": [
    "# bert_embedder = RuBertTiny2Embedder(\n",
    "#     max_length=512,\n",
    "#     batch_size=8,\n",
    "#     pooling_type='weighted',\n",
    "#     learning_rate=2.2e-5,\n",
    "#     num_epochs=5      # just 1 epoch for testing\n",
    "# )\n",
    "# bert_embedder.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wy5r9vFOL6bx",
    "outputId": "95104def-776e-413b-ce09-e41b90619d7f"
   },
   "outputs": [],
   "source": [
    "def filter_tokens(token_weights):\n",
    "    filtered = []\n",
    "    for token, weight in token_weights:\n",
    "        # Приводим токен к нижнему регистру перед проверками\n",
    "        lower_token = token.lower()\n",
    "        # Исключаем: служебные токены, короткие, цифры\n",
    "        if (token not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]']) and \\\n",
    "           (len(lower_token.replace('##', '')) >= 4) and \\\n",
    "           (not lower_token.replace('##', '').isdigit()):\n",
    "            filtered.append((lower_token.replace('##', ''), weight))\n",
    "    return filtered\n",
    "\n",
    "token_stats = {}\n",
    "sample_texts = X_train.head(1000).tolist()\n",
    "\n",
    "for text in tqdm(sample_texts, desc=\"Анализ важности токенов\"):\n",
    "    tw = bert_embedder.get_token_importance([text])[0]\n",
    "    for tok, w in filter_tokens(tw):\n",
    "        # Используем lower() для ключа в словаре\n",
    "        lower_tok = tok.lower()\n",
    "        entry = token_stats.setdefault(lower_tok, {'count':0, 'total_weight':0.0})\n",
    "        entry['count'] += 1\n",
    "        entry['total_weight'] += w\n",
    "\n",
    "print(f\"Collected stats for {len(token_stats)} tokens.\")\n",
    "if not token_stats:\n",
    "    raise RuntimeError(\n",
    "        \"token_stats is empty — your get_token_importance() returned nothing!\"\n",
    "    )\n",
    "\n",
    "# Build DataFrame\n",
    "df_tokens = pd.DataFrame.from_dict(token_stats, orient='index')\n",
    "df_tokens['avg_weight'] = df_tokens['total_weight'] / df_tokens['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zErEr8dHbx3a",
    "outputId": "af6b631d-8e20-4b30-d2b9-7a985e145379"
   },
   "outputs": [],
   "source": [
    "df_tokens = pd.DataFrame.from_dict(token_stats, orient='index')\n",
    "df_tokens['avg_weight'] = df_tokens['total_weight'] / df_tokens['count']\n",
    "\n",
    "df_tokens.sort_values(['avg_weight'], ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "umWQtocDMllY",
    "outputId": "a979a08d-071b-496c-8e95-c90cfd80710a"
   },
   "outputs": [],
   "source": [
    "df_tokens.sort_values(['avg_weight'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "5tjG4pc1UGj-",
    "outputId": "8dfc4794-5404-444f-a77c-79f996d8335f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. Визуализация\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='avg_weight', y=df_tokens.index, data=df_tokens, palette='viridis')\n",
    "plt.title('Топ-30 важных токенов (длина ≥4, без служебных)')\n",
    "plt.xlabel('Средний вес внимания')\n",
    "plt.ylabel('Токен')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_tokens.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 7. Вывод таблицы\n",
    "print(\"Таблица топ-30 токенов:\")\n",
    "display(df_tokens[['count', 'avg_weight']].style.background_gradient(cmap='Blues'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaLVtMqvTL8O",
    "outputId": "9143a7de-d36c-4980-8d61-aee0c9365ca1"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # Импортируем tqdm для прогресс-бара\n",
    "import re\n",
    "\n",
    "def filter_tokens(token_weights):\n",
    "    filtered = []\n",
    "    for token, weight in token_weights:\n",
    "        lower_token = token.lower()\n",
    "        # Менее строгие условия\n",
    "        if (token not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]', '[sep]', '[SEP]'])  and \\\n",
    "           (len(lower_token.replace('##', '')) >= 2) and \\\n",
    "           (not re.fullmatch(r'\\d+', lower_token.replace('##', ''))):  # Только чисто цифровые\n",
    "            clean_token = lower_token.replace('##', '')\n",
    "            filtered.append((clean_token, weight))\n",
    "    return filtered\n",
    "\n",
    "\n",
    "token_stats = defaultdict(lambda: {'count': 0, 'total_weight': 0.0})\n",
    "sample_texts = X_train.tolist()\n",
    "\n",
    "# Добавляем tqdm для отображения прогресса\n",
    "for text in tqdm(sample_texts, desc=\"Анализ токенов\", unit=\"текст\"):\n",
    "    tw = bert_embedder.get_token_importance([text])[0]\n",
    "    current_word = []\n",
    "    current_weight = 0\n",
    "    for tok, w in tw:\n",
    "        if tok.startswith('##'):\n",
    "            current_word.append(tok[2:])\n",
    "            current_weight += w\n",
    "        else:\n",
    "            if current_word:  # Сохраняем предыдущее слово\n",
    "                full_word = ''.join(current_word)\n",
    "                if len(full_word) >= 3:  # Проверяем длину полного слова\n",
    "                    entry = token_stats[full_word.lower()]\n",
    "                    entry['count'] += 1\n",
    "                    entry['total_weight'] += current_weight\n",
    "            current_word = [tok.lower()]\n",
    "            current_weight = w\n",
    "    # Не забыть добавить последнее слово\n",
    "    if current_word and len(''.join(current_word)) >= 3:\n",
    "        full_word = ''.join(current_word)\n",
    "        entry = token_stats[full_word.lower()]\n",
    "        entry['count'] += 1\n",
    "        entry['total_weight'] += current_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "id": "VKtEN9bhmDK0",
    "outputId": "52a2d6ce-a7ac-4b0d-d37d-8c656b167007"
   },
   "outputs": [],
   "source": [
    "df_tokens = pd.DataFrame.from_dict(token_stats, orient='index')\n",
    "\n",
    "\n",
    "df_tokens[df_tokens.index == 'торг']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3fhgqxMJYplb",
    "outputId": "51a5e573-d850-4f0a-8bc1-ab8c80d796e0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns  # Добавляем импорт seaborn с псевдонимом sns\n",
    "\n",
    "# Создаем DataFrame с результатами\n",
    "df_tokens = pd.DataFrame.from_dict(token_stats, orient='index')\n",
    "df_tokens = df_tokens[df_tokens.index!='[sep]']\n",
    "df_tokens['avg_weight'] = df_tokens['total_weight'] / df_tokens['count']\n",
    "df_tokens = df_tokens.sort_values('avg_weight', ascending=False)\n",
    "\n",
    "# Фильтруем редкие токены\n",
    "df_tokens = df_tokens[df_tokens['count'] > 20]  # Только токены, встречающиеся >5 раз\n",
    "\n",
    "# Топ-30 токенов\n",
    "top_tokens = df_tokens.head(30)\n",
    "print(\"Топ-30 важных токенов:\")\n",
    "print(top_tokens[['count', 'avg_weight']].to_markdown())\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# График важности токенов\n",
    "ax = sns.barplot(x='avg_weight', y=top_tokens.index, data=top_tokens, palette=\"viridis\")\n",
    "plt.title('Top-30 most important tokens by their average weight', fontsize=14)\n",
    "plt.xlabel('Average weight', fontsize=12)\n",
    "plt.ylabel('Token', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Добавляем значения на график\n",
    "for i, (_, row) in enumerate(top_tokens.iterrows()):\n",
    "    ax.text(row['avg_weight'] + 0.005, i, f\"{row['avg_weight']:.3f}\",\n",
    "            va='center', fontsize=9)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Дополнительная визуализация: зависимость веса от частоты\n",
    "plt.figure(figsize=(12, 8))  # Increased size for better readability\n",
    "sns.scatterplot(x='count', y='avg_weight', size='count', sizes=(20, 200),\n",
    "                data=top_tokens, hue=top_tokens.index, legend=False)\n",
    "plt.title('Token Importance vs. Frequency', fontsize=14)\n",
    "plt.xlabel('Token Frequency (count)', fontsize=12)\n",
    "plt.ylabel('Average Weight', fontsize=12)\n",
    "\n",
    "# Add labels for all points with count >= 20\n",
    "for line in range(top_tokens.shape[0]):\n",
    "    if top_tokens['count'].iloc[line] >= 20:\n",
    "        plt.text(top_tokens['count'].iloc[line] + 3,  # X-offset for readability\n",
    "                top_tokens['avg_weight'].iloc[line],\n",
    "                top_tokens.index[line],\n",
    "                horizontalalignment='left',\n",
    "                verticalalignment='center',\n",
    "                size=9,\n",
    "                color='black',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gb4ehFxrnR5c",
    "outputId": "65ebe2b3-c7f1-424e-cf43-18905b0e46b8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_avg_tokens(texts, tokenizer, max_length=512):\n",
    "    \"\"\"Вычисляет среднее количество токенов в текстах после токенизации с прогресс-баром\"\"\"\n",
    "    token_counts = []\n",
    "\n",
    "    # Добавляем прогресс-бар\n",
    "    for text in tqdm(texts, desc=\"Токенизация текстов\", unit=\"текст\"):\n",
    "        # Токенизируем текст без учета padding\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=False,\n",
    "            return_length=True\n",
    "        )\n",
    "        token_counts.append(encoded['length'])\n",
    "\n",
    "    avg_tokens = np.mean(token_counts)\n",
    "    print(f\"\\nРезультаты токенизации:\")\n",
    "    print(f\"Среднее количество токенов на текст: {avg_tokens:.1f} (±{np.std(token_counts):.1f})\")\n",
    "    print(f\"Минимум: {np.min(token_counts)}, Максимум: {np.max(token_counts)}\")\n",
    "    print(f\"25-й перцентиль: {np.percentile(token_counts, 25):.1f}\")\n",
    "    print(f\"Медиана: {np.median(token_counts):.1f}\")\n",
    "    print(f\"75-й перцентиль: {np.percentile(token_counts, 75):.1f}\")\n",
    "    return avg_tokens\n",
    "\n",
    "# Пример использования:\n",
    "avg_tokens = calculate_avg_tokens(\n",
    "    texts=X_train,  # Ваши текстовые данные\n",
    "    tokenizer=bert_embedder.tokenizer,  # Токенизатор из вашего RuBertTiny2Embedder\n",
    "    max_length=bert_embedder.max_length  # Макс. длина из конфига\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV9n5BSrdQ7o"
   },
   "source": [
    "# MTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t-PNoV9VdSl4",
    "outputId": "9e226ad9-954b-4543-a969-b66db17f3b1f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Функции для загрузки данных\n",
    "def load_summary_metrics(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    metrics = {}\n",
    "    for _, row in df.iterrows():\n",
    "        if 'orig' in row['type']:\n",
    "            metrics[row['metric']] = {\n",
    "                'mean': row['mean'],\n",
    "                'std': row['std'],\n",
    "                'conf_interval': row['conf_interval'],\n",
    "                'value': row['value']\n",
    "            }\n",
    "    return metrics\n",
    "\n",
    "def load_cv_metrics(filepath):\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    return data['orig_metrics']\n",
    "\n",
    "def load_predictions(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Укажите базовый путь к вашим данным\n",
    "base_path = \"/content/drive/MyDrive/price_prediction_data/\"\n",
    "\n",
    "# Обновляем пути к файлам в словаре models\n",
    "models = {\n",
    "    'no_text': {\n",
    "        'summary': os.path.join(base_path, \"Results_pca/XGBR/metrics/summary_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False.csv\"),\n",
    "        'cv': os.path.join(base_path, \"Results_pca/XGBR/metrics/cv_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False.json\"),\n",
    "        'predictions': os.path.join(base_path, \"Results_pca/XGBR/metrics/predictions_test_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False.csv\")\n",
    "    },\n",
    "    'all_text': {\n",
    "        'summary': os.path.join(base_path, \"manual_text_features/XGBR/metrics/summary_metrics_price_prediction_mannual_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\"),\n",
    "        'cv': os.path.join(base_path, \"manual_text_features/XGBR/metrics/cv_metrics_price_prediction_mannual_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.json\"),\n",
    "        'predictions': os.path.join(base_path, \"manual_text_features/XGBR/metrics/predictions_test_price_prediction_mannual_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\")\n",
    "    },\n",
    "    'neg_text': {\n",
    "        'summary': os.path.join(base_path, \"manual_text_features_negative/XGBR/metrics/summary_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\"),\n",
    "        'cv': os.path.join(base_path, \"manual_text_features_negative/XGBR/metrics/cv_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.json\"),\n",
    "        'predictions': os.path.join(base_path, \"manual_text_features_negative/XGBR/metrics/predictions_test_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Проверяем существование файлов перед загрузкой\n",
    "for model_name in models:\n",
    "    for file_type in ['summary', 'cv', 'predictions']:\n",
    "        file_path = models[model_name][file_type]\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Файл не найден: {file_path}\")\n",
    "            # Можно либо пропустить этот файл, либо завершить выполнение\n",
    "            # В данном примере просто выводим предупреждение\n",
    "\n",
    "\n",
    "# Загружаем данные\n",
    "for model_name in models:\n",
    "    models[model_name]['summary_metrics'] = load_summary_metrics(models[model_name]['summary'])\n",
    "    models[model_name]['cv_metrics'] = load_cv_metrics(models[model_name]['cv'])\n",
    "    models[model_name]['predictions'] = load_predictions(models[model_name]['predictions'])\n",
    "\n",
    "# Функция для бутстрапа\n",
    "def bootstrap_metric(y_true, y_pred, metric_func, n_bootstrap=1000, ci=95):\n",
    "    bootstrapped_scores = []\n",
    "    rng = np.random.RandomState(42)\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        score = metric_func(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(bootstrapped_scores)\n",
    "    ci_lower = np.percentile(bootstrapped_scores, (100 - ci) / 2)\n",
    "    ci_upper = np.percentile(bootstrapped_scores, ci + (100 - ci) / 2)\n",
    "    return mean_score, (ci_lower, ci_upper)\n",
    "\n",
    "# Функции метрик\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def medape(y_true, y_pred):\n",
    "    return 100 * np.median(np.abs(y_pred - y_true) / np.abs(y_true))\n",
    "\n",
    "# Создаем таблицы сравнения\n",
    "def create_comparison_table(models, metric_names, test_data=False):\n",
    "    comparison = []\n",
    "\n",
    "    for metric in metric_names:\n",
    "        row = {'metric': metric}\n",
    "\n",
    "        for model_name in models:\n",
    "            if test_data:\n",
    "                # Для тестовых данных используем бутстрап\n",
    "                y_true = models[model_name]['predictions']['true_price'].values\n",
    "                y_pred = models[model_name]['predictions']['predicted_price'].values\n",
    "\n",
    "                if metric == 'r2':\n",
    "                    metric_func = lambda y_true, y_pred: 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "                elif metric == 'rmse':\n",
    "                    metric_func = lambda y_true, y_pred: np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "                elif metric == 'smape':\n",
    "                    metric_func = smape\n",
    "                elif metric == 'medape':\n",
    "                    metric_func = medape\n",
    "\n",
    "                mean_val, (ci_lower, ci_upper) = bootstrap_metric(y_true, y_pred, metric_func)\n",
    "                conf_interval = (mean_val - ci_lower)  # полуширина интервала\n",
    "                row[model_name] = f\"{mean_val:.4f} ± {conf_interval:.4f}\"\n",
    "            else:\n",
    "                # Для CV берем из загруженных данных\n",
    "                mean_val = models[model_name]['cv_metrics'][metric]['mean']\n",
    "                conf_interval = models[model_name]['cv_metrics'][metric]['conf_interval']\n",
    "                row[model_name] = f\"{mean_val:.4f} ± {conf_interval:.4f}\"\n",
    "\n",
    "        comparison.append(row)\n",
    "\n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "# Функция для проверки статистической значимости\n",
    "def add_statistical_significance(df, models, metric_names, test_data=False):\n",
    "    for metric in metric_names:\n",
    "        # Сравниваем модели с текстовыми фичами с моделью без текстовых фичей\n",
    "        for compare_model in ['all_text', 'neg_text']:\n",
    "            if test_data:\n",
    "                # Для тестовых данных используем пермутационный тест\n",
    "                y_true_no = models['no_text']['predictions']['true_price'].values\n",
    "                y_pred_no = models['no_text']['predictions']['predicted_price'].values\n",
    "                y_true_comp = models[compare_model]['predictions']['true_price'].values\n",
    "                y_pred_comp = models[compare_model]['predictions']['predicted_price'].values\n",
    "\n",
    "                if metric == 'r2':\n",
    "                    metric_func = lambda y_true, y_pred: 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "                elif metric == 'rmse':\n",
    "                    metric_func = lambda y_true, y_pred: np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "                elif metric == 'smape':\n",
    "                    metric_func = smape\n",
    "                elif metric == 'medape':\n",
    "                    metric_func = medape\n",
    "\n",
    "                # Пермутационный тест\n",
    "                n_permutations = 1000\n",
    "                original_diff = metric_func(y_true_comp, y_pred_comp) - metric_func(y_true_no, y_pred_no)\n",
    "\n",
    "                combined = np.concatenate([y_pred_no, y_pred_comp])\n",
    "                perm_diffs = []\n",
    "\n",
    "                for _ in range(n_permutations):\n",
    "                    np.random.shuffle(combined)\n",
    "                    perm_pred_no = combined[:len(y_pred_no)]\n",
    "                    perm_pred_comp = combined[len(y_pred_no):]\n",
    "\n",
    "                    perm_diff = metric_func(y_true_comp, perm_pred_comp) - metric_func(y_true_no, perm_pred_no)\n",
    "                    perm_diffs.append(perm_diff)\n",
    "\n",
    "                p_value = (np.abs(np.array(perm_diffs)) >= np.abs(original_diff)).mean()\n",
    "            else:\n",
    "                # Для CV используем t-test для парных выборок\n",
    "                no_text_values = models['no_text']['cv_metrics'][metric]['values']\n",
    "                comp_values = models[compare_model]['cv_metrics'][metric]['values']\n",
    "                _, p_value = stats.ttest_rel(no_text_values, comp_values)\n",
    "\n",
    "            # Добавляем звездочки в зависимости от p-value\n",
    "            if p_value < 0.001:\n",
    "                sig = '***'\n",
    "            elif p_value < 0.01:\n",
    "                sig = '**'\n",
    "            elif p_value < 0.05:\n",
    "                sig = '*'\n",
    "            else:\n",
    "                sig = ''\n",
    "\n",
    "            # Добавляем обозначение значимости в таблицу\n",
    "            mask = df['metric'] == metric\n",
    "            df.loc[mask, compare_model] = df.loc[mask, compare_model].values[0] + sig\n",
    "\n",
    "    return df\n",
    "\n",
    "# Метрики для сравнения\n",
    "metric_names = ['r2', 'rmse', 'smape', 'medape']\n",
    "\n",
    "# Создаем таблицу для CV метрик\n",
    "cv_comparison = create_comparison_table(models, metric_names, test_data=False)\n",
    "cv_comparison = add_statistical_significance(cv_comparison, models, metric_names, test_data=False)\n",
    "\n",
    "# Создаем таблицу для тестовых метрик\n",
    "test_comparison = create_comparison_table(models, metric_names, test_data=True)\n",
    "test_comparison = add_statistical_significance(test_comparison, models, metric_names, test_data=True)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Сравнение метрик кросс-валидации (оригинальная шкала):\")\n",
    "print(cv_comparison.to_markdown(index=False))\n",
    "\n",
    "print(\"\\nСравнение метрик тестовой выборки (оригинальная шкала):\")\n",
    "print(test_comparison.to_markdown(index=False))\n",
    "\n",
    "# Визуализация результатов\n",
    "def plot_metric_comparison(df, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics = df['metric'].unique()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        for model in ['no_text', 'all_text', 'neg_text']:\n",
    "            # Удаляем символы статистической значимости перед преобразованием\n",
    "            val_err = df[df['metric'] == metric][model].str.replace(r'[*]+', '', regex=True).str.split('±')\n",
    "            vals = val_err.str[0].astype(float)\n",
    "            errs = val_err.str[1].str.strip().astype(float)  # Удаляем пробелы в начале\n",
    "\n",
    "            plt.bar(model, vals, yerr=errs, capsize=5, label=model)\n",
    "\n",
    "        plt.title(metric)\n",
    "        plt.ylabel('Value')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_metric_comparison(cv_comparison, \"CV Metrics Comparison\")\n",
    "plot_metric_comparison(test_comparison, \"Test Metrics Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lDRoJwazgjLW",
    "outputId": "4eeefc41-d7a1-4891-ba9d-3a6ab1467711"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# ======================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# ======================\n",
    "\n",
    "def load_summary_metrics(filepath):\n",
    "    \"\"\"Load summary metrics from CSV file\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    metrics = {}\n",
    "    for _, row in df.iterrows():\n",
    "        if 'orig' in row['type']:\n",
    "            metrics[row['metric']] = {\n",
    "                'mean': row['mean'],\n",
    "                'std': row['std'],\n",
    "                'conf_interval': row['conf_interval'],\n",
    "                'value': row['value']\n",
    "            }\n",
    "    return metrics\n",
    "\n",
    "def load_cv_metrics(filepath):\n",
    "    \"\"\"Load cross-validation metrics from JSON file\"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    return data['orig_metrics']\n",
    "\n",
    "def load_predictions(filepath):\n",
    "    \"\"\"Load prediction results from CSV file\"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "# =================\n",
    "# CONFIGURATION\n",
    "# =================\n",
    "\n",
    "# Base path configuration\n",
    "base_path = \"/content/drive/MyDrive/price_prediction_data/\"\n",
    "\n",
    "# Model configuration with proper paths\n",
    "models = {\n",
    "    'Baseline': {\n",
    "        'summary': os.path.join(base_path, \"Results_pca/XGBR/metrics/summary_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False.csv\"),\n",
    "        'cv': os.path.join(base_path, \"Results_pca/XGBR/metrics/cv_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False.json\"),\n",
    "        'predictions': os.path.join(base_path, \"Results_pca/XGBR/metrics/predictions_test_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False.csv\")\n",
    "    },\n",
    "    'All Text Features': {\n",
    "        'summary': os.path.join(base_path, \"manual_text_features/XGBR/metrics/summary_metrics_price_prediction_mannual_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\"),\n",
    "        'cv': os.path.join(base_path, \"manual_text_features/XGBR/metrics/cv_metrics_price_prediction_mannual_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.json\"),\n",
    "        'predictions': os.path.join(base_path, \"manual_text_features/XGBR/metrics/predictions_test_price_prediction_mannual_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\")\n",
    "    },\n",
    "    'Negative Text Only': {\n",
    "        'summary': os.path.join(base_path, \"manual_text_features_negative/XGBR/metrics/summary_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\"),\n",
    "        'cv': os.path.join(base_path, \"manual_text_features_negative/XGBR/metrics/cv_metrics_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.json\"),\n",
    "        'predictions': os.path.join(base_path, \"manual_text_features_negative/XGBR/metrics/predictions_test_price_prediction_XGBRegressor_categorical-only_none_pca_False_gate_False_mtf_True_.csv\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# =====================\n",
    "# METRIC CALCULATIONS\n",
    "# =====================\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def medape(y_true, y_pred):\n",
    "    \"\"\"Median Absolute Percentage Error\"\"\"\n",
    "    return 100 * np.median(np.abs(y_pred - y_true) / np.abs(y_true))\n",
    "\n",
    "def bootstrap_metric(y_true, y_pred, metric_func, n_bootstrap=1000, ci=95):\n",
    "    \"\"\"Calculate bootstrap confidence intervals\"\"\"\n",
    "    bootstrapped_scores = []\n",
    "    rng = np.random.RandomState(42)\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        score = metric_func(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(bootstrapped_scores)\n",
    "    ci_lower = np.percentile(bootstrapped_scores, (100 - ci) / 2)\n",
    "    ci_upper = np.percentile(bootstrapped_scores, ci + (100 - ci) / 2)\n",
    "    return mean_score, (ci_lower, ci_upper)\n",
    "\n",
    "# =====================\n",
    "# DATA PROCESSING\n",
    "# =====================\n",
    "\n",
    "def create_comparison_table(models, metric_names, test_data=False):\n",
    "    \"\"\"Create comparison table with confidence intervals\"\"\"\n",
    "    comparison = []\n",
    "    metric_labels = {\n",
    "        'r2': 'R² Score',\n",
    "        'rmse': 'RMSE (million RUB)',  # Updated to indicate millions of rubles\n",
    "        'smape': 'SMAPE (%)',\n",
    "        'medape': 'MedAPE (%)'\n",
    "    }\n",
    "\n",
    "    for metric in metric_names:\n",
    "        row = {'Metric': metric_labels.get(metric, metric)}\n",
    "\n",
    "        for model_name in models:\n",
    "            if test_data:\n",
    "                y_true = models[model_name]['predictions']['true_price'].values\n",
    "                y_pred = models[model_name]['predictions']['predicted_price'].values\n",
    "\n",
    "                if metric == 'r2':\n",
    "                    metric_func = lambda y_true, y_pred: 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "                elif metric == 'rmse':\n",
    "                    metric_func = lambda y_true, y_pred: np.sqrt(np.mean((y_true - y_pred)**2)) / 1e6  # Convert to millions\n",
    "                elif metric == 'smape':\n",
    "                    metric_func = smape\n",
    "                elif metric == 'medape':\n",
    "                    metric_func = medape\n",
    "\n",
    "                mean_val, (ci_lower, ci_upper) = bootstrap_metric(y_true, y_pred, metric_func)\n",
    "                conf_interval = (mean_val - ci_lower)\n",
    "                row[model_name] = f\"{mean_val:.2f} ± {conf_interval:.2f}\"  # Reduced to 2 decimal places for RUB millions\n",
    "            else:\n",
    "                if metric == 'rmse':\n",
    "                    # Convert CV RMSE values to millions\n",
    "                    mean_val = models[model_name]['cv_metrics'][metric]['mean'] / 1e6\n",
    "                    conf_interval = models[model_name]['cv_metrics'][metric]['conf_interval'] / 1e6\n",
    "                else:\n",
    "                    mean_val = models[model_name]['cv_metrics'][metric]['mean']\n",
    "                    conf_interval = models[model_name]['cv_metrics'][metric]['conf_interval']\n",
    "                row[model_name] = f\"{mean_val:.2f} ± {conf_interval:.2f}\"\n",
    "\n",
    "        comparison.append(row)\n",
    "\n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "def add_statistical_significance(df, models, metric_names, test_data=False):\n",
    "    \"\"\"Add statistical significance markers to results\"\"\"\n",
    "    p_values_df = df.copy()\n",
    "    metric_labels = {\n",
    "        'r2': 'R² Score',\n",
    "        'rmse': 'RMSE (million RUB)',\n",
    "        'smape': 'SMAPE (%)',\n",
    "        'medape': 'MedAPE (%)'\n",
    "    }\n",
    "\n",
    "    for metric in metric_names:\n",
    "        current_metric_label = metric_labels.get(metric, metric)\n",
    "        p_values = {'Metric': current_metric_label}\n",
    "\n",
    "        for compare_model in ['All Text Features', 'Negative Text Only']:\n",
    "            if test_data:\n",
    "                y_true_base = models['Baseline']['predictions']['true_price'].values\n",
    "                y_pred_base = models['Baseline']['predictions']['predicted_price'].values\n",
    "                y_true_comp = models[compare_model]['predictions']['true_price'].values\n",
    "                y_pred_comp = models[compare_model]['predictions']['predicted_price'].values\n",
    "\n",
    "                if metric == 'r2':\n",
    "                    metric_func = lambda y_true, y_pred: 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "                elif metric == 'rmse':\n",
    "                    # Convert to millions for comparison\n",
    "                    metric_func = lambda y_true, y_pred: np.sqrt(np.mean((y_true - y_pred)**2)) / 1e6\n",
    "                elif metric == 'smape':\n",
    "                    metric_func = smape\n",
    "                elif metric == 'medape':\n",
    "                    metric_func = medape\n",
    "\n",
    "                n_permutations = 1000\n",
    "                original_diff = metric_func(y_true_comp, y_pred_comp) - metric_func(y_true_base, y_pred_base)\n",
    "                combined = np.concatenate([y_pred_base, y_pred_comp])\n",
    "                perm_diffs = []\n",
    "\n",
    "                for _ in tqdm(range(n_permutations), desc=f\"{metric} ({compare_model})\"):\n",
    "                    np.random.shuffle(combined)\n",
    "                    perm_pred_base = combined[:len(y_pred_base)]\n",
    "                    perm_pred_comp = combined[len(y_pred_base):]\n",
    "                    perm_diff = metric_func(y_true_comp, perm_pred_comp) - metric_func(y_true_base, perm_pred_base)\n",
    "                    perm_diffs.append(perm_diff)\n",
    "\n",
    "                p_value = (np.abs(np.array(perm_diffs)) >= np.abs(original_diff)).mean()\n",
    "            else:\n",
    "                base_values = models['Baseline']['cv_metrics'][metric]['values']\n",
    "                comp_values = models[compare_model]['cv_metrics'][metric]['values']\n",
    "\n",
    "                if metric == 'rmse':\n",
    "                    # Convert to millions for CV comparison\n",
    "                    base_values = [x / 1e6 for x in base_values]\n",
    "                    comp_values = [x / 1e6 for x in comp_values]\n",
    "\n",
    "                _, p_value = stats.ttest_rel(base_values, comp_values)\n",
    "\n",
    "            p_values[compare_model] = p_value\n",
    "\n",
    "            if p_value < 0.001:\n",
    "                sig = '***'\n",
    "            elif p_value < 0.01:\n",
    "                sig = '**'\n",
    "            elif p_value < 0.05:\n",
    "                sig = '*'\n",
    "            else:\n",
    "                sig = ''\n",
    "\n",
    "            mask = df['Metric'] == current_metric_label\n",
    "            df.loc[mask, compare_model] = df.loc[mask, compare_model].values[0] + sig\n",
    "\n",
    "        if metric == metric_names[0]:\n",
    "            p_values_df = pd.DataFrame([p_values])\n",
    "        else:\n",
    "            p_values_df = pd.concat([p_values_df, pd.DataFrame([p_values])], ignore_index=True)\n",
    "\n",
    "    return df, p_values_df\n",
    "\n",
    "# =====================\n",
    "# VISUALIZATION\n",
    "# =====================\n",
    "\n",
    "def plot_metric_comparison(df, p_values_df, title, figure_num):\n",
    "    \"\"\"Create comparison plots with significance markers\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    metrics = df['Metric'].unique()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        for model in ['Baseline', 'All Text Features', 'Negative Text Only']:\n",
    "            clean_val = df[df['Metric'] == metric][model].str.replace(r'[*]+', '', regex=True)\n",
    "            val_err = clean_val.str.split('±')\n",
    "            vals = val_err.str[0].astype(float)\n",
    "            errs = val_err.str[1].str.strip().astype(float)\n",
    "\n",
    "            colors = {\n",
    "                'Baseline': '#3498db',\n",
    "                'All Text Features': '#e74c3c',\n",
    "                'Negative Text Only': '#27ae60'\n",
    "            }\n",
    "\n",
    "            bar = plt.bar(model, vals, yerr=errs, capsize=5, label=model,\n",
    "                         color=colors[model], alpha=0.7, width=0.6)\n",
    "\n",
    "            if model in ['All Text Features', 'Negative Text Only']:\n",
    "                p_val = p_values_df[p_values_df['Metric'] == metric][model].values[0]\n",
    "                if p_val < 0.05:\n",
    "                    height = vals.values[0] + errs.values[0] + 0.05 * vals.values[0]\n",
    "                    plt.text(model, height, f'p={p_val:.3f}', ha='center',\n",
    "                            va='bottom', fontsize=9, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        plt.title(f\"{metric}\", fontsize=12)\n",
    "\n",
    "        # Add million RUB label for RMSE\n",
    "        ylabel = 'Value (million RUB)' if 'RMSE' in metric else 'Value'\n",
    "        plt.ylabel(ylabel, fontsize=10)\n",
    "\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.xticks(rotation=15)\n",
    "\n",
    "    plt.suptitle(f\"Figure {figure_num}: {title}\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Save figure to buffer for HTML report\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=120, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "\n",
    "# =====================\n",
    "# REPORT GENERATION\n",
    "# =====================\n",
    "\n",
    "def create_html_report(cv_table, test_table, cv_p_values, test_p_values, fig1_base64, fig2_base64):\n",
    "    \"\"\"Generate comprehensive HTML report\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Model Comparison Report: Text Features Analysis</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "                margin: 20px;\n",
    "                line-height: 1.6;\n",
    "                color: #333;\n",
    "            }}\n",
    "            h1 {{\n",
    "                color: #2c3e50;\n",
    "                border-bottom: 2px solid #3498db;\n",
    "                padding-bottom: 10px;\n",
    "                text-align: center;\n",
    "            }}\n",
    "            h2 {{\n",
    "                color: #2980b9;\n",
    "                margin-top: 40px;\n",
    "                border-left: 4px solid #3498db;\n",
    "                padding-left: 10px;\n",
    "            }}\n",
    "            h3 {{\n",
    "                color: #16a085;\n",
    "                margin-top: 25px;\n",
    "            }}\n",
    "            .table-container {{\n",
    "                margin: 20px 0;\n",
    "                overflow-x: auto;\n",
    "            }}\n",
    "            table {{\n",
    "                border-collapse: collapse;\n",
    "                width: 100%;\n",
    "                margin-bottom: 25px;\n",
    "                box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
    "            }}\n",
    "            th, td {{\n",
    "                border: 1px solid #ddd;\n",
    "                padding: 12px;\n",
    "                text-align: left;\n",
    "            }}\n",
    "            th {{\n",
    "                background-color: #3498db;\n",
    "                color: white;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            tr:nth-child(even) {{\n",
    "                background-color: #f8f9fa;\n",
    "            }}\n",
    "            tr:hover {{\n",
    "                background-color: #f1f1f1;\n",
    "            }}\n",
    "            .significance {{\n",
    "                color: #e74c3c;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .section {{\n",
    "                margin-bottom: 40px;\n",
    "                background-color: #f9f9f9;\n",
    "                padding: 20px;\n",
    "                border-radius: 5px;\n",
    "                box-shadow: 0 2px 4px rgba(0,0,0,0.05);\n",
    "            }}\n",
    "            .metric-title {{\n",
    "                font-weight: bold;\n",
    "                color: #2c3e50;\n",
    "            }}\n",
    "            .model-name {{\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .baseline {{\n",
    "                color: #3498db;\n",
    "            }}\n",
    "            .all-text {{\n",
    "                color: #e74c3c;\n",
    "            }}\n",
    "            .neg-text {{\n",
    "                color: #27ae60;\n",
    "            }}\n",
    "            .table-caption {{\n",
    "                font-weight: bold;\n",
    "                margin-bottom: 10px;\n",
    "                text-align: left;\n",
    "                color: #2c3e50;\n",
    "            }}\n",
    "            .figure-container {{\n",
    "                text-align: center;\n",
    "                margin: 30px 0;\n",
    "            }}\n",
    "            .figure-caption {{\n",
    "                font-weight: bold;\n",
    "                margin-top: 10px;\n",
    "                text-align: center;\n",
    "                color: #2c3e50;\n",
    "            }}\n",
    "            .interpretation {{\n",
    "                background-color: #e8f4fc;\n",
    "                padding: 15px;\n",
    "                border-radius: 5px;\n",
    "                margin: 15px 0;\n",
    "            }}\n",
    "            .note {{\n",
    "                font-style: italic;\n",
    "                color: #7f8c8d;\n",
    "                font-size: 0.9em;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Model Comparison Report: Impact of Text Features</h1>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>1. Cross-Validation Results</h2>\n",
    "\n",
    "            <div class=\"table-container\">\n",
    "                <div class=\"table-caption\">Table 1: Cross-Validation Metrics</div>\n",
    "                <p class=\"note\">Note: RMSE values are shown in millions of RUB (₽)</p>\n",
    "                {cv_table.to_html(index=False, escape=False, classes=\"metric-table\")}\n",
    "            </div>\n",
    "\n",
    "            <div class=\"table-container\">\n",
    "                <div class=\"table-caption\">Table 2: Statistical Significance (p-values)</div>\n",
    "                {cv_p_values.to_html(index=False, float_format=\"%.4f\", classes=\"p-value-table\")}\n",
    "            </div>\n",
    "\n",
    "            <div class=\"figure-container\">\n",
    "                <img src=\"data:image/png;base64,{fig1_base64}\" width=\"800\">\n",
    "                <div class=\"figure-caption\">Figure 1: Cross-Validation Metrics Comparison</div>\n",
    "                <p class=\"note\">Note: RMSE values are shown in millions of RUB (₽)</p>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>2. Test Set Results</h2>\n",
    "\n",
    "            <div class=\"table-container\">\n",
    "                <div class=\"table-caption\">Table 3: Test Set Metrics</div>\n",
    "                <p class=\"note\">Note: RMSE values are shown in millions of RUB (₽)</p>\n",
    "                {test_table.to_html(index=False, escape=False, classes=\"metric-table\")}\n",
    "            </div>\n",
    "\n",
    "            <div class=\"table-container\">\n",
    "                <div class=\"table-caption\">Table 4: Statistical Significance (p-values)</div>\n",
    "                {test_p_values.to_html(index=False, float_format=\"%.4f\", classes=\"p-value-table\")}\n",
    "            </div>\n",
    "\n",
    "            <div class=\"figure-container\">\n",
    "                <img src=\"data:image/png;base64,{fig2_base64}\" width=\"800\">\n",
    "                <div class=\"figure-caption\">Figure 2: Test Set Metrics Comparison</div>\n",
    "                <p class=\"note\">Note: RMSE values are shown in millions of RUB (₽)</p>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>3. Interpretation Guide</h2>\n",
    "\n",
    "            <div class=\"interpretation\">\n",
    "                <h3>Statistical Significance Indicators:</h3>\n",
    "                <ul>\n",
    "                    <li><span class=\"significance\">*</span> p-value &lt; 0.05 (significant)</li>\n",
    "                    <li><span class=\"significance\">**</span> p-value &lt; 0.01 (very significant)</li>\n",
    "                    <li><span class=\"significance\">***</span> p-value &lt; 0.001 (highly significant)</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "\n",
    "            <h3>Model Descriptions:</h3>\n",
    "            <ul>\n",
    "                <li><span class=\"model-name baseline\">Baseline</span>: Model without any text features</li>\n",
    "                <li><span class=\"model-name all-text\">All Text Features</span>: Model incorporating all available text features</li>\n",
    "                <li><span class=\"model-name neg-text\">Negative Text Only</span>: Model using only negative sentiment text features</li>\n",
    "            </ul>\n",
    "\n",
    "            <h3>Performance Metrics Explained:</h3>\n",
    "            <ul>\n",
    "                <li><span class=\"metric-title\">R² Score</span>: Measures the proportion of variance explained (0-1, higher is better)</li>\n",
    "                <li><span class=\"metric-title\">RMSE (million RUB)</span>: Root Mean Square Error - absolute measure of prediction errors in millions of rubles (lower is better)</li>\n",
    "                <li><span class=\"metric-title\">SMAPE (%)</span>: Symmetric Mean Absolute Percentage Error - percentage accuracy measure (lower is better)</li>\n",
    "                <li><span class=\"metric-title\">MedAPE (%)</span>: Median Absolute Percentage Error - robust percentage accuracy measure (lower is better)</li>\n",
    "            </ul>\n",
    "\n",
    "            <div class=\"interpretation\">\n",
    "                <h3>Key Findings:</h3>\n",
    "                <p>The analysis compares the impact of different text feature configurations on model performance.\n",
    "                Statistical significance markers indicate whether the differences from the baseline model are meaningful.</p>\n",
    "                <p>All monetary values (RMSE) are presented in millions of Russian rubles (₽) for better readability.</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# =====================\n",
    "# MAIN EXECUTION\n",
    "# =====================\n",
    "\n",
    "# Load and verify data\n",
    "print(\"Loading data...\")\n",
    "for model_name in models:\n",
    "    print(f\"\\nProcessing {model_name} model:\")\n",
    "    for file_type in ['summary', 'cv', 'predictions']:\n",
    "        file_path = models[model_name][file_type]\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: File not found - {file_path}\")\n",
    "        else:\n",
    "            print(f\"Found {file_type} data\")\n",
    "\n",
    "    try:\n",
    "        models[model_name]['summary_metrics'] = load_summary_metrics(models[model_name]['summary'])\n",
    "        models[model_name]['cv_metrics'] = load_cv_metrics(models[model_name]['cv'])\n",
    "        models[model_name]['predictions'] = load_predictions(models[model_name]['predictions'])\n",
    "        print(\"Data loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "\n",
    "# Metric configuration\n",
    "metric_names = ['r2', 'rmse', 'smape', 'medape']\n",
    "\n",
    "print(\"\\nRunning analysis...\")\n",
    "# Cross-validation analysis\n",
    "cv_comparison, cv_p_values = add_statistical_significance(\n",
    "    create_comparison_table(models, metric_names, test_data=False),\n",
    "    models, metric_names, test_data=False\n",
    ")\n",
    "\n",
    "# Test set analysis\n",
    "test_comparison, test_p_values = add_statistical_significance(\n",
    "    create_comparison_table(models, metric_names, test_data=True),\n",
    "    models, metric_names, test_data=True\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "# Generate visualizations and get base64 strings\n",
    "fig1_base64 = plot_metric_comparison(cv_comparison, cv_p_values, \"Cross-Validation Metrics Comparison\", 1)\n",
    "fig2_base64 = plot_metric_comparison(test_comparison, test_p_values, \"Test Set Metrics Comparison\", 2)\n",
    "\n",
    "print(\"Creating HTML report...\")\n",
    "# Create and display HTML report\n",
    "html_report = create_html_report(cv_comparison, test_comparison, cv_p_values, test_p_values, fig1_base64, fig2_base64)\n",
    "with open('model_comparison_report.html', 'w') as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "print(\"\\nAnalysis complete! Displaying results...\")\n",
    "display(HTML(html_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EYODpyW_8vc"
   },
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xvpfJrJ2vp-_"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
